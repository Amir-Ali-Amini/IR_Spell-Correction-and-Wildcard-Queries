{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an Information Retrieval System with Spelling Correction and Wildcard Queries\n",
    "\n",
    "## AmirAli Amini - 610399102\n",
    "\n",
    "#### HW2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# توضیحات مسئله و چالش ها و بهبود ها\n",
    "\n",
    "بزرگ ترین چالش این مسئله ساخت درخت ترای بود که با ساختن دو کلاس یکی برای هر نود و یکی برای کل درخت انجام شد\n",
    "\n",
    "در این سوال تمام جایگشت های دوری یک رشته را با $ در انتهای آن به درست اضافه میکنم که بتوان طبق الگوریتم گفته شده در کتاب وایدکارت ها را بدست اورد؛\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## کتابخانه ها \n",
    "\n",
    "###  from nltk import word_tokenize :\n",
    "از این کتابخانه برای تکنایز کردن داده ها به این دلیل که توکنایز کردن دیتا سریع تر میشه استفاده کردم\n",
    "\n",
    "###  from nltk.corpus import stopwords :\n",
    "از این کتابخانه برای دریافت استاپینگ ورد های زبان انگلیسی استفاده کردم\n",
    "\n",
    "###  import string:\n",
    "از این کتابخانه برای دریافت پانچویشن های زبان انگلیسی استفاده کردم\n",
    "\n",
    "###  import numpy as np:\n",
    "از این کتابخانه برای جمع یک عدد با تمام اعضای یک آرایه استفاده کردم\n",
    "\n",
    "###  import copy:\n",
    "از این کتابخانه برای دیپ کپی کردن ارایه استفاده کردم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 61] Connection\n",
      "[nltk_data]     refused>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 61]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords # a library to tokenize input texts\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') # stopping word in English language\n",
    "\n",
    "import string # using to remove punctuation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "در این سوال فقط به توضیح توابع اضافه شده نسبت به تمرین سری اول میپردازم\n",
    "\n",
    "### node:\n",
    "این کلاس به صورت مشخص ایمپلیمنت کننده هر برگ درخت ترای است\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node : #class node for nodes of try tree\n",
    "    def __init__(self,isWord=False, myChar=None , myWord=None):\n",
    "        self.myChar = myChar\n",
    "        self.myWord = myWord\n",
    "        self.nodes = {} # children\n",
    "        self.seenChars = []  # chars in children\n",
    "        self.isAWrod = isWord # boolean to check if this node indicate a valid word or not\n",
    "    \n",
    "    def addNode(self ,char , isWord = False):  # add a child to self and return new node\n",
    "        if (char not in self.seenChars):\n",
    "            newNode = node(myChar=char , myWord=self.myWord+char if self.myWord else char , isWord=isWord)\n",
    "            self.nodes[char] = newNode\n",
    "            self.seenChars.append(char)\n",
    "            return newNode\n",
    "        if isWord:\n",
    "            self.nodes[char].makeWord()\n",
    "        return self.nodes[char]\n",
    "        \n",
    "    def makeWord(self): # set self node to a valid word\n",
    "        self.isAWrod=True\n",
    "\n",
    "    def getWord (self):\n",
    "        return self.myWord\n",
    "    \n",
    "    def getIsWord (self):\n",
    "        return self.isAWrod\n",
    "\n",
    "    def getChar (self):\n",
    "        return self.myChar\n",
    "    \n",
    "    def includesChar(self, char):\n",
    "        return char in self.seenChars\n",
    "    \n",
    "    def getSeenNodes (self):\n",
    "        return [self.nodes[char] for char in self.seenChars]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tryTree\n",
    "این کلاس هم درخت تری را ایمپلیمنت میکند و توضیح توابع در کامنت ها نوشته شده است\n",
    "\n",
    "تنها نکته مهم استفاده از \n",
    "BFS \n",
    "برای پیمایش زیر درخت هر نود است که یک نود را میگیرد و زیر درختش را پیمایش میکند و همه نود ها را برمیگرداند\n",
    "\n",
    "\n",
    "\n",
    "#### getBFSWords:\n",
    "\n",
    "این تابع کلمات ولید در نود های برگردانده شده از bfs را برمیگرداند\n",
    "\n",
    "#### BFSPrefix: \n",
    "تابع بی اف اس پریفیکس هم یک پیشوند برای تمام کلماتی که در درخت میخواهیم پیدا کنیم ورودی میگیرد سپس به نود ان پیشوند رفته و بی اف اس را از آن نود شروع میکند\n",
    "\n",
    "\n",
    "#### find:\n",
    "تابع فاند عملیات پیدا کردن وایلد کارت را رو درخت طبق الگوریتم گفته شده انجام میدهذ به این صورت که پریفیکس کلمه (مقدار قبل از ستاره) را در درخت جست و جو میکند سپس کلمات بازگشت داده شده را میچرخاند با علامت دلار در اخر آنها قرار گیرد.\n",
    "\n",
    "برای دو ستاره نیز برای یک ستاره این کار را انجام میدهد و برای ستاره دوم بین تمام کلمات بازگشتی آنهایی که دارای مقدار بین دو ستاره هستند را برمیگرداند \n",
    "\n",
    "توجه شود که علارت به صورتی میچرخد که یک ستاره در انتها و بین ستاره ها کمترین حروف قرار گیرد و اینکار برای افزایش سرعت سرچ میباشد\n",
    "\n",
    "همچنین در ابتدا تابع سرچ وایلد کارت به انتها ورودی کی علامت دلار اضافه میشود که نشان از انتها کلمه است.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class tryTree :\n",
    "    def __init__ (self):\n",
    "        self.root = node()\n",
    "\n",
    "    def insertWord(self, word): # insert a word to tree by adding all its chars respectively\n",
    "        currentNode  = self.root\n",
    "        for char in range(len(word)): \n",
    "            currentNode = currentNode.addNode(word[char] , isWord = char == len(word)-1)\n",
    "\n",
    "\n",
    "    def insertWordPermutation (self,inputWord): # insert all permutation of a word to the tree by adding $ to end of it\n",
    "        word = inputWord+\"$\"\n",
    "        for index in range(len(word)):\n",
    "            self.insertWord(word[index:]+word[:index])\n",
    "\n",
    "    def BFS(self , node): # BFS traversal of a subtree\n",
    "        result = [node]\n",
    "        for child in node.getSeenNodes():\n",
    "            result+= self.BFS(child)\n",
    "        return result\n",
    "    \n",
    "    def getBFSWords (self , node): # get valid word from result of BFS\n",
    "        ls = self.BFS(node)\n",
    "        return [node.getWord() for node in ls if node.getIsWord()]\n",
    "    \n",
    "    def prefixBFS (self, prefix= ''): # find prefix node and start BFS from it\n",
    "        currentNode = self.root\n",
    "        for char in range(len(prefix)): \n",
    "            currentNode = currentNode.addNode(prefix[char])\n",
    "        ls = self.getBFSWords(currentNode)\n",
    "        return ls\n",
    "\n",
    "    \n",
    "    def find(self, inputWord): # find wild cards in try tree\n",
    "        word = inputWord\n",
    "        if ('*' not in word):\n",
    "            return []\n",
    "        # if inputWord[-1] != \"*\":\n",
    "        word +='$'\n",
    "        startCount = word.count('*')\n",
    "        if (startCount ==1):\n",
    "            index = word.find('*')\n",
    "            prefix = word[index+1:]+word[:index]\n",
    "            ls = self.prefixBFS(prefix=prefix)\n",
    "            st = set()\n",
    "            for w in ls :\n",
    "                index = w.find('$')\n",
    "                st.add(w[index+1:]+w[:index])\n",
    "            return list(st)\n",
    "        elif(startCount==2):\n",
    "            while (word[-1]!='*'):\n",
    "                word = word[-1:]+word[:-1]\n",
    "            splitWord = word[:-1].split('*')\n",
    "            if len(splitWord[0])< len(splitWord[1]):\n",
    "                splitWord[0], splitWord[1] = splitWord[1],splitWord[0]\n",
    "            \n",
    "            ls = [i for i in self.prefixBFS(prefix=splitWord[0]) if splitWord[1] in i[len(splitWord[0]):]]\n",
    "\n",
    "            st = set()\n",
    "            for word in ls :\n",
    "                index = word.find('$')\n",
    "                st.add(word[index+1:]+word[:index])\n",
    "            return list(st)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def prnt (self,node=None):\n",
    "        root = self.root\n",
    "        if (node):\n",
    "            root = node\n",
    "        print (self.getBFSWords(root) )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDED FUNCTIONS:\n",
    "\n",
    "\n",
    "#### findDistance:\n",
    "پیدا کردن فاصله دو کلمه با استفاده از الگوریتم گفته شده در اسلاید ها\n",
    "\n",
    "\n",
    "#### spellCheckingSingleWord:\n",
    "مقایسه کلمه ورودی با همه کلمات پستینگ لیست و بازگرداندن کمترین فاصله\n",
    "\n",
    "در ایتدا چک میشود اگر کلمه در در پستینگ لیست وجود داشت آنرا برمیگردانیم\n",
    "\n",
    "\n",
    "#### spellCheckingExpressions:\n",
    "برای تک تک ورد های عبارت ورودی اسپل چکینگ را انجام میدهد و ترکیب آنها را برمیگرداند\n",
    "\n",
    "#### findQueryWord:\n",
    "با استقاده از تابع فایند درخت سرچ وایلد کارد را برای یک کلمه انجام میدهد\n",
    "\n",
    "\n",
    "#### findQuery:\n",
    "عبارت را همانند تمرین قبل تجزیه کرده و برای کلمات ستاره دار وایلد کارد و برای کلمات بدون ستاره اسپل چکینگ را انجام میدهد سپس تمام کویری ها ممکن را میسازد و با اجتماع گرفتن از همه نتیجه ها، نتیجه نهایی را برمیگرداند\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class searchEngine:\n",
    "    def __init__(self , debug = False) -> None: # constructor of class\n",
    "        self.debug = debug\n",
    "        self.postingList =[]\n",
    "        self.subPostingLists=[]\n",
    "        self.codedPostingLit=[]\n",
    "        self.files=[] \n",
    "        self.stop = set(stopwords.words('english') + list(string.punctuation)) # all extra expression which should ignore\n",
    "        self.tryTree = tryTree()\n",
    "\n",
    "        # structure of postingList : list of {word : nameOfWord , docs :[list of {doc:nameOfDocument , indexes: indexes of the word if this document}]}\n",
    "\n",
    "\n",
    "    # binary search to find a word in posting list\n",
    "    def searchPostingList(self, word,whichPostingList=-1):\n",
    "        if(whichPostingList!=-1):\n",
    "            if (len(self.subPostingLists)<= whichPostingList):\n",
    "                self.subPostingLists.append([])\n",
    "            postingList = self.subPostingLists[whichPostingList]\n",
    "        else :\n",
    "            postingList=self.postingList\n",
    "        s= 0 \n",
    "        e = len(postingList)\n",
    "        if e <=0 :\n",
    "            return 0\n",
    "        e-=1\n",
    "        while (1):\n",
    "            if (e-s < 2):\n",
    "                if (postingList[e][\"word\"] < word):\n",
    "                    return e+1\n",
    "                if (postingList[e][\"word\"] == word):\n",
    "                    return e\n",
    "                if (postingList[s][\"word\"] >= word):\n",
    "                    return s\n",
    "\n",
    "                return e\n",
    "            mid = (s+e)/2\n",
    "            mid = int(mid)\n",
    "            if (word<postingList[mid][\"word\"]):\n",
    "                e=mid\n",
    "            elif (word> postingList[mid][\"word\"]):\n",
    "                s = mid\n",
    "            else :\n",
    "                return mid\n",
    "            \n",
    "        # {word:str, indexes:list(int)}\n",
    "    # binary search to find a word in each dictionary\n",
    "    def searchDictionary(self, word,ls):\n",
    "        s= 0 \n",
    "        e = len(ls)\n",
    "        if e <=0 :\n",
    "            return 0\n",
    "        e-=1\n",
    "        while (1):\n",
    "            if (e-s < 2):\n",
    "                if (ls[e][\"doc\"] < word):\n",
    "                    return e+1\n",
    "                if (ls[e][\"doc\"] == word):\n",
    "                    return e\n",
    "                if (ls[s][\"doc\"] >= word):\n",
    "                    return s\n",
    "\n",
    "                return e\n",
    "            mid = (s+e)/2\n",
    "            mid = int(mid)\n",
    "            if (word<ls[mid][\"doc\"]):\n",
    "                e=mid\n",
    "            elif (word> ls[mid][\"doc\"]):\n",
    "                s = mid\n",
    "            else :\n",
    "                return mid\n",
    "\n",
    "\n",
    "            \n",
    "        # {doc:number, indexes:list(int)}\n",
    "\n",
    "\n",
    "\n",
    "    def addToPostingList(self, tokenizedText: list[str],docIndex:int,whichPostingList=-1): # add tokenized word in posting list \n",
    "        if(whichPostingList!=-1):\n",
    "            if (len(self.subPostingLists)<= whichPostingList):\n",
    "                self.subPostingLists.append([])\n",
    "            postingList = self.subPostingLists[whichPostingList]\n",
    "        else :\n",
    "            postingList=self.postingList\n",
    "        for i in range(len(tokenizedText)):\n",
    "            word = tokenizedText[i]\n",
    "            index = self.searchPostingList(word,whichPostingList=whichPostingList) # find index of word in posting list\n",
    "            if (len(postingList)>index): # check if index is not larger than posting list (if word is bigger that all words, search function returns len(postingList)+1)\n",
    "                if (postingList[index][\"word\"] == word): # check if index is the index of the word\n",
    "                    if (postingList[index][\"docs\"][-1][\"doc\"] == docIndex): # if we have already added the document index \n",
    "                        postingList[index][\"docs\"][-1][\"indexes\"].append(i) # as we read tokens in order of their index, we need to add token in end of the list\n",
    "\n",
    "                    else:\n",
    "                        postingList[index][\"docs\"].append({\"doc\":docIndex,\"indexes\":[i] }) # if we have not already added the document and dou to the fact that they are read in order of their index, we can easily add append new one in end of the list \n",
    "\n",
    "                else :\n",
    "                    postingList[index:index]= [({\"word\":word , \"docs\":[{\"doc\":docIndex , \"indexes\":[i]}]})] # word is bigger that all other words => we can append it to end of the list\n",
    "\n",
    "            else :\n",
    "                postingList.append({\"word\":word , \"docs\":[{\"doc\":docIndex , \"indexes\":[i]}]}) # we have not already added the word and dou to the fact that they are read in order of their index, we can easily add append new one in end of the list\n",
    "\n",
    "            self.tryTree.insertWordPermutation(word)\n",
    "\n",
    "\n",
    "    def input (self, filePath: list[str]): # input paths of inputs\n",
    "        for i in range(len(filePath)): # for files in input\n",
    "            file = open(filePath[i],'r',encoding='cp1252') # open the file\n",
    "            text = file.read() # read the file\n",
    "            file.close()  # close the file\n",
    "            # tokenize text and ignore stopping words using nltk library \n",
    "            tokenizedText = [word for word in word_tokenize(text.lower(),preserve_line=False) if word not in self.stop] \n",
    "            print (f'document {i+1} : {filePath[i]}')\n",
    "            # print(tokenizedText)\n",
    "            self.addToPostingList(tokenizedText , i+1 ,whichPostingList=int(i/5 )) # i indicates to index of document we are reading\n",
    "        self.mergePostingLists()\n",
    "\n",
    "    def numberToGamaCode(self, n):  # new function for homework 3\n",
    "        binaryNumber = bin(n)[2:]\n",
    "        ans = \"\"\n",
    "        for _ in range (len(binaryNumber)):\n",
    "            ans+=\"1\"\n",
    "        ans+=\"0\"+binaryNumber[1:]\n",
    "        return ans\n",
    "\n",
    "\n",
    "\n",
    "    def listToGamaCode(self, ls , startingIndex = 0):  # new function for homework 3\n",
    "        temp = \"\"\n",
    "        s = startingIndex\n",
    "        for item in ls:\n",
    "            temp+=self.numberToGamaCode(item-s)\n",
    "            s = item\n",
    "        return temp\n",
    "    \n",
    "    def gamaCodeToNumber(self,gamaCode):\n",
    "        oneCounter=0\n",
    "        for i in gamaCode:\n",
    "            if i == '1':\n",
    "                oneCounter+=1\n",
    "            else: break\n",
    "        if oneCounter==1:return 1\n",
    "        return int(\"1\"+gamaCode[oneCounter+1:],2)\n",
    "        \n",
    "\n",
    "    \n",
    "    def gamaCodeToList(self,gamaCode):\n",
    "        temp = []\n",
    "        s=0\n",
    "        e=0\n",
    "        oneCounter = 0\n",
    "        while(s!=len(gamaCode)):\n",
    "            if gamaCode[e] ==\"1\":\n",
    "                e+=1\n",
    "            else :\n",
    "                e=(e-s)*2+s\n",
    "                temp.append(gamaCode[s:e])\n",
    "                s=e\n",
    "        \n",
    "        return list(map(self.gamaCodeToNumber,temp))\n",
    "\n",
    "\n",
    "\n",
    "    def mergePostingLists (self): # new function for homework 3\n",
    "        postingList = self.postingList\n",
    "        codedPostingList = self.codedPostingLit\n",
    "        for ls in range(len(self.subPostingLists)):\n",
    "            for item in self.subPostingLists[ls]:\n",
    "                index = self.searchPostingList(item[\"word\"])\n",
    "                word = item[\"word\"]\n",
    "                index = self.searchPostingList(word) # find index of word in posting list\n",
    "                if (len(postingList)>index): # check if index is not larger than posting list (if word is bigger that all words, search function returns len(postingList)+1)\n",
    "                    if (postingList[index][\"word\"] == word): # check if index is the index of the word\n",
    "                        postingList[index][\"docs\"] += item[\"docs\"]\n",
    "                        codedPostingList[index][\"docs\"]+=self.listToGamaCode([document[\"doc\"] for document in item[\"docs\"] ],codedPostingList[index][\"lastSeenIndex\"])\n",
    "                        codedPostingList[\"lastSeenIndex\"]:item[\"docs\"][-1][\"doc\"]\n",
    "\n",
    "                    else :\n",
    "                        postingList[index:index]= [copy.deepcopy(item)]\n",
    "                        codedPostingList[index:index]=[{\"word\":item[\"word\"],\"docs\": self.listToGamaCode([document[\"doc\"] for document in item[\"docs\"] ]) , \"lastSeenIndex\":item[\"docs\"][-1][\"doc\"]}]\n",
    "\n",
    "                else :\n",
    "                    postingList.append(copy.deepcopy(item)) # we have not already added the word and dou to the fact that they are read in order of their index, we can easily add append new one in end of the list\n",
    "                    codedPostingList.append({\"word\":item[\"word\"],\"docs\": self.listToGamaCode([document[\"doc\"] for document in item[\"docs\"] ]) , \"lastSeenIndex\":item[\"docs\"][-1][\"doc\"]})\n",
    "\n",
    "\n",
    "    def findWord(self, word): # this function use our binary search function to find word in posting list and if the word is not included in the list, returns -1\n",
    "        index = self.searchPostingList(word)\n",
    "        if (index< len(self.postingList)):\n",
    "            if (self.postingList[index][\"word\"] == word):\n",
    "                return index # real index of the word\n",
    "        return -1 # word is not in the posting list\n",
    "\n",
    "\n",
    "    def find(self , query:str): # split the query and find the result \n",
    "        splitQuery = query.lower().split()\n",
    "        if (len(splitQuery)==1): # query is only one word\n",
    "            index = self.findWord(splitQuery[0])\n",
    "            if (index>-1):\n",
    "                return [ i[\"doc\"] for i in self.postingList[index][\"docs\"]]\n",
    "            return []\n",
    "\n",
    "        else: \n",
    "            index1 = self.findWord(splitQuery[0])\n",
    "            index2 = self.findWord(splitQuery[2])\n",
    "            if splitQuery[1] in [\"and\" ,\"or\", \"AND\", \"OR\"]: # boolean condition\n",
    "                \n",
    "                if (splitQuery[1] in [\"and\",\"AND\"]): # and condition\n",
    "                    if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                        docs1 =  set([ i[\"doc\"] for i in self.postingList[index1][\"docs\"]]) # find document of word one \n",
    "                        docs2 =  set([ i[\"doc\"] for i in self.postingList[index2][\"docs\"]]) # find document of word two \n",
    "                        # print(self.postingList[index1][\"docs\"] , self.postingList[index2][\"docs\"])\n",
    "                        return list(docs1.intersection(docs2)) # make intersection of two results\n",
    "                    return []\n",
    "                    \n",
    "                if (splitQuery[1] in [\"or\",\"OR\"]): # or condition\n",
    "                    if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                        docs1 =  set([ i[\"doc\"] for i in self.postingList[index1][\"docs\"]])\n",
    "                        docs2 =  set([ i[\"doc\"] for i in self.postingList[index2][\"docs\"]])\n",
    "                        # print(self.postingList[index1][\"docs\"] , self.postingList[index2][\"docs\"])\n",
    "                        return list(docs1.union(docs2))\n",
    "                    if(index1!=-1): return [ i[\"doc\"] for i in self.postingList[index1][\"docs\"]] # check if there is result for first word\n",
    "                    return [ i[\"doc\"] for i in self.postingList[index2][\"docs\"]] # check if there is result for second word\n",
    "\n",
    "            else : # near condition\n",
    "                nearNumber = int(splitQuery[1].split('/')[1]) # find near \n",
    "                if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                    docs1 =   {i[\"doc\"]:i[\"indexes\"] for i in self.postingList[index1][\"docs\"]} # find document of word one and make dictionary for result\n",
    "                    docs2 =   {i[\"doc\"]:i[\"indexes\"] for i in self.postingList[index2][\"docs\"]} # find document of word two and make dictionary for result\n",
    "\n",
    "                    result = [] \n",
    "                    keysOfDocs2 = docs2.keys()\n",
    "                    for key, value in docs1.items():\n",
    "                        if (key in keysOfDocs2):\n",
    "                            d1 = np.array(value) # indexes of word in first document\n",
    "                            d2 = np.array(copy.deepcopy(docs2[key])) # indexes of word in second document\n",
    "                            distanceMatrix = np.array([[abs(i - j )for i in d1] for j in d2]) # distance matrix\n",
    "                            if (distanceMatrix.min() <= nearNumber): # check validation\n",
    "                                result.append(key)\n",
    "\n",
    "\n",
    "                            # example of distanceMatrix\n",
    "                            # \n",
    "                            # [[ 0  1  2 10 11]\n",
    "                            # [ 1  0  1  9 10]\n",
    "                            # [ 2  1  0  8  9]\n",
    "                            # [10  9  8  0  1]\n",
    "                            # [11 10  9  1  0]]\n",
    "\n",
    "\n",
    "                    return result\n",
    "                return []\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HW2 : codes of homework 2\n",
    "\n",
    "    def findDistance (self,inputWord, baseWord ,printOff=True ): # this function is used to find distance of each word if posting list with input word\n",
    "        inWord = \" \"+inputWord\n",
    "        bWord = \" \"+baseWord\n",
    "        distanceMatrix = [[0]* len(bWord) for _ in range(len(inWord))] # quantified the matrix\n",
    "        for i in range(len(bWord)):\n",
    "            distanceMatrix[0][i]=i\n",
    "        for i in range(len(inWord)):\n",
    "            distanceMatrix[i][0]=i\n",
    "\n",
    "        for row in range(1,len(inWord)): # fill the matrix using dynamic programming algorithm\n",
    "            for column in range(1,len(bWord)):\n",
    "                deleteScore = distanceMatrix[row-1][column] +1\n",
    "                insertScore = distanceMatrix[row][column-1] +1\n",
    "                copyOrReplaceScore = distanceMatrix[row-1][column-1] \n",
    "                if bWord[column] != inWord[row]:\n",
    "                    copyOrReplaceScore+=1\n",
    "                distanceMatrix[row][column] = min(deleteScore , insertScore , copyOrReplaceScore)\n",
    "                self.debugPrint (f'min is {distanceMatrix[row][column]} for {(deleteScore , insertScore , copyOrReplaceScore)} {row}, {column}' ,printOff=printOff)\n",
    " \n",
    "        self.debugPrint(np.array(distanceMatrix),printOff=printOff)\n",
    "        return distanceMatrix[-1][-1] # return the result\n",
    "\n",
    "\n",
    "\n",
    "    def spellCheckingSingleWord (self,inputWord,printOff=True): # call \"def findDistance ()\" for all words in posting list with input word\n",
    "        index = self.searchPostingList(inputWord) # check if spell is correct (word exists in posting list)\n",
    "        if (index< len(self.postingList)):\n",
    "            if (self.postingList[index][\"word\"] == inputWord):\n",
    "                return [inputWord]\n",
    "            \n",
    "        allWords = [item['word'] for item in self.postingList]\n",
    "        distances = [[] for _ in range(100) ]\n",
    "        for baseWord in allWords:\n",
    "            currentDistance = self.findDistance(inputWord, baseWord ,printOff=True) # call \"def findDistance ()\" for all words in posting list with input \n",
    "            distances[currentDistance].append(baseWord) # add new distance to distances list\n",
    "        self.debugPrint (distances,isMatrix=True, printOff=printOff)\n",
    "        for item in distances:\n",
    "            if(len(item)):\n",
    "                return item\n",
    "        return []\n",
    "            \n",
    "\n",
    "    def spellCheckingExpression(self, expression, printOff = True): # split the expression and check spell for all parts the concat them together\n",
    "        # tokenize input and remove stop words and ponctuations\n",
    "        tokenizedExpression= [word for word in word_tokenize(expression.lower(),preserve_line=False) if word not in self.stop] \n",
    "        answersLists = [] # list of all near words for each token\n",
    "        for word in tokenizedExpression:\n",
    "            answersLists.append(self.spellCheckingSingleWord(word))\n",
    "        self.debugPrint(answersLists, printOff=printOff)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for ls in answersLists: # multiply lists founded before\n",
    "            temp =[]\n",
    "            if (len(results)):\n",
    "                for wordResult in results:\n",
    "                    for word in ls:\n",
    "                        temp.append(f'{wordResult} {word}')\n",
    "                results = temp\n",
    "            else :results = copy.copy(ls)\n",
    "\n",
    "        self.debugPrint(results, printOff=printOff)\n",
    "        return results\n",
    "    \n",
    "\n",
    "    def findQueryWord (self, word ) : # I do my wild search in my tree class\n",
    "        wildcardFind = self.tryTree.find(word)\n",
    "        return wildcardFind\n",
    "\n",
    "\n",
    "    def findQuery (self, query ,printQueries=True) :\n",
    "        splitQuery = query.lower().split()\n",
    "        if ('*' not in query): # check if query is a wild card\n",
    "            # query does not contains any wildcard\n",
    "            if (len(splitQuery) == 3): # complicated query\n",
    "                # spell checking for both of sides\n",
    "                spellCheckFind1 = self.spellCheckingExpression(splitQuery[0]) \n",
    "                spellCheckFind2 = self.spellCheckingExpression(splitQuery[2])\n",
    "                results = set()\n",
    "                for spellItem1 in spellCheckFind1:\n",
    "                    for spellItem2 in spellCheckFind2: # make results\n",
    "                        if printQueries: print (f'query : {spellItem2} {splitQuery[1]} {spellItem1}')\n",
    "                        results = results.union(set(self.find(f'{spellItem2} {splitQuery[1]} {spellItem1}')))\n",
    "                    return list(results)\n",
    "                return self.find(query)\n",
    "            else: # simple query - just spellchecking \n",
    "                spellCheckFind = self.spellCheckingExpression(splitQuery[0])\n",
    "                results = set()\n",
    "                for item in spellCheckFind:\n",
    "                    results=results.union(set(self.find(item)))\n",
    "                return list(results)\n",
    "\n",
    "        # query contains some wildcards \n",
    "\n",
    "        if (len(splitQuery) == 3): # complicated query\n",
    "            if '*' not in splitQuery[0]: \n",
    "                splitQuery[0],splitQuery[2] = splitQuery[2],splitQuery[0] # move wildcard to left side\n",
    "\n",
    "            wildcardFind = self.tryTree.find(splitQuery[0]) # search wildcard\n",
    "            spellCheckFind = self.spellCheckingExpression(splitQuery[2]) # checking spell\n",
    "            self.debugPrint(wildcardFind)\n",
    "            results = set()\n",
    "            for spellItem in spellCheckFind: # make results\n",
    "                # search \n",
    "                for wildItem in wildcardFind:\n",
    "                    if printQueries: print (f'query : {wildItem} {splitQuery[1]} {spellItem}')\n",
    "                    results = results.union(set(self.find(f'{wildItem} {splitQuery[1]} {spellItem}')))\n",
    "                # print(f'{item} {splitQuery[1]} {splitQuery[2]}' ,set(self.find(f'{item} {splitQuery[1]} {splitQuery[2]}')) ,results)\n",
    "            self.debugPrint(wildcardFind,isMatrix=False)\n",
    "            self.debugPrint(splitQuery,isMatrix=True)\n",
    "            return list(results)\n",
    "        \n",
    "        else: # simple query with one wildcard\n",
    "            wildcardFind = self.tryTree.find(splitQuery[0]) \n",
    "            results = set()\n",
    "            for item in wildcardFind:\n",
    "                results = results.union(set(self.find(item))) # find all documents contain result of wild card searching\n",
    "            return list(results)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "            \n",
    "\n",
    "            \n",
    "    def debugPrint(self, value ,isMatrix = False, printOff=False):\n",
    "        if (self.debug and not printOff):\n",
    "            if (isMatrix):\n",
    "                for row in value:\n",
    "                    print(row)\n",
    "            else:\n",
    "                print(value)\n",
    "        \n",
    "\n",
    "\n",
    "    def prnt(self):\n",
    "        for i in self.postingList:\n",
    "            print(i)\n",
    "\n",
    "    def printGamaCodePostingList(self):\n",
    "        print(\"gama code posting list is : \")\n",
    "        for i in self.codedPostingLit:\n",
    "            print(i)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1 : document1.txt\n",
      "document 2 : document2.txt\n",
      "document 3 : document3.txt\n"
     ]
    }
   ],
   "source": [
    "test = searchEngine()\n",
    "\n",
    "test.input(['document1.txt','document2.txt','document3.txt'])\n",
    "\n",
    "# test.prnt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gama code posting list is : \n",
      "{'word': 'another', 'docs': '110010', 'lastSeenIndex': 3}\n",
      "{'word': 'boolean', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'capabilities', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'contains', 'docs': '101100', 'lastSeenIndex': 3}\n",
      "{'word': 'content', 'docs': '110010', 'lastSeenIndex': 3}\n",
      "{'word': 'different', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'document', 'docs': '101010', 'lastSeenIndex': 3}\n",
      "{'word': 'example', 'docs': '101010', 'lastSeenIndex': 3}\n",
      "{'word': 'important', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'indexed', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'indexing', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'processed', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'relevant', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'retrieval', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'search', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'several', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'simple', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'test', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'words', 'docs': '10', 'lastSeenIndex': 1}\n"
     ]
    }
   ],
   "source": [
    "test.printGamaCodePostingList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docs WildCards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['docs/Jerry Decided To Buy a Gun.txt', 'docs/Rentals at the Oceanside Community.txt', 'docs/Gasoline Prices Hit Record High.txt', 'docs/Cloning Pets.txt', 'docs/Crazy Housing Prices.txt', 'docs/Man Injured at Fast Food Place.txt', 'docs/A Festival of Books.txt', 'docs/Food Fight Erupted in Prison.txt', 'docs/Better To Be Unlucky.txt', 'docs/Sara Went Shopping.txt', 'docs/Freeway Chase Ends at Newsstand.txt', 'docs/Trees Are a Threat.txt', 'docs/A Murder-Suicide.txt', 'docs/Happy and Unhappy Renters.txt', 'docs/Pulling Out Nine Tons of Trash.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory_input = ['docs/'+path for path in os.listdir('docs') if path[-3:] == 'txt']\n",
    "print(directory_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1 : docs/Jerry Decided To Buy a Gun.txt\n",
      "document 2 : docs/Rentals at the Oceanside Community.txt\n",
      "document 3 : docs/Gasoline Prices Hit Record High.txt\n",
      "document 4 : docs/Cloning Pets.txt\n",
      "document 5 : docs/Crazy Housing Prices.txt\n",
      "document 6 : docs/Man Injured at Fast Food Place.txt\n",
      "document 7 : docs/A Festival of Books.txt\n",
      "document 8 : docs/Food Fight Erupted in Prison.txt\n",
      "document 9 : docs/Better To Be Unlucky.txt\n",
      "document 10 : docs/Sara Went Shopping.txt\n",
      "document 11 : docs/Freeway Chase Ends at Newsstand.txt\n",
      "document 12 : docs/Trees Are a Threat.txt\n",
      "document 13 : docs/A Murder-Suicide.txt\n",
      "document 14 : docs/Happy and Unhappy Renters.txt\n",
      "document 15 : docs/Pulling Out Nine Tons of Trash.txt\n"
     ]
    }
   ],
   "source": [
    "test_directory = searchEngine()\n",
    "test_directory.input(directory_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1 : document1.txt\n",
      "document 2 : document2.txt\n",
      "document 3 : document3.txt\n"
     ]
    }
   ],
   "source": [
    "test = searchEngine()\n",
    "\n",
    "test.input(['document1.txt','document2.txt','document3.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : example and content\n",
      "[2, 3]\n",
      "docs/Rentals at the Oceanside Community.txt\n",
      "docs/Gasoline Prices Hit Record High.txt\n"
     ]
    }
   ],
   "source": [
    "result = test.findQuery(\"exa*le AND contrnt\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gama code posting list is : \n",
      "{'word': '.38', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': '1', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': '1,000', 'docs': '1011110011', 'lastSeenIndex': 1}\n",
      "{'word': '10', 'docs': '101010110011001101', 'lastSeenIndex': 5}\n",
      "{'word': '1000', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '1000x', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '100x', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '10x', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '11', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': '110,000', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '12', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': '120', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '120,000', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '15', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': '15-percent', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': '150', 'docs': '11101111110000', 'lastSeenIndex': 7}\n",
      "{'word': '1992.', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': '1993', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': '1995', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': '1x', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '2,000', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': '2.09', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': '2.14', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': '2.22', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': '20', 'docs': '110111110010', 'lastSeenIndex': 3}\n",
      "{'word': '20,000', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': '200', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': '230,000', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': '24-year-old', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': '25', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '25,000', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': '25,000.', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': '280', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': '29.95', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': '3', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': '3.50', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': '30', 'docs': '101100111011', 'lastSeenIndex': 3}\n",
      "{'word': '300', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': '3037', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': '39.95', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': '4,000', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': '4,000.residents', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': '5', 'docs': '11011100', 'lastSeenIndex': 5}\n",
      "{'word': '50', 'docs': '11100011101011110001', 'lastSeenIndex': 4}\n",
      "{'word': '50,000', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': '500', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': '500x', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '50th', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': '50x', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '510,000', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': '6,000', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': '60', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': '65-year-old', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': '7', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': '70,000', 'docs': '1110011100', 'lastSeenIndex': 5}\n",
      "{'word': '70-year-old', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': '74-year-old', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': '75', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': '75,000', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': '76', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '79-year-old', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': '87', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': '9', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': '9,000', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': '90', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': '99', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': '9:00', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'a.m.', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'able', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'abuse', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'accidentally', 'docs': '10111001', 'lastSeenIndex': 1}\n",
      "{'word': 'according', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'acres', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'actually', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'administration', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'afghan', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'agency', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'ago', 'docs': '1100101110001011110010', 'lastSeenIndex': 3}\n",
      "{'word': 'aid', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'alan', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'allen', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'allotments', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'allow', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'allowing', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'almost', 'docs': '110010111100101100', 'lastSeenIndex': 3}\n",
      "{'word': 'alone', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'along', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'already', 'docs': '11001100110111110001', 'lastSeenIndex': 4}\n",
      "{'word': 'also', 'docs': '1010111011111100011101', 'lastSeenIndex': 2}\n",
      "{'word': 'altadena', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'although', 'docs': '11101111110000', 'lastSeenIndex': 7}\n",
      "{'word': 'always', 'docs': '1110011111000010', 'lastSeenIndex': 5}\n",
      "{'word': 'ambulance', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'american', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'ammunition', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'among', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'amount', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'amounts', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'amputated', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'angelenos', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'angeles', 'docs': '111011111000', 'lastSeenIndex': 7}\n",
      "{'word': 'anniversary', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'annual', 'docs': '111011111011', 'lastSeenIndex': 7}\n",
      "{'word': 'another', 'docs': '110011110100', 'lastSeenIndex': 2}\n",
      "{'word': 'antennas', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'anyone', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'anything', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'apartment', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'apartments', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'apparent', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'appears', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'appliances', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'apply', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'approved', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'april', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'arcadia', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'area', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'arizona', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'around', 'docs': '11011100', 'lastSeenIndex': 5}\n",
      "{'word': 'arrived', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'asked', 'docs': '110111110001', 'lastSeenIndex': 3}\n",
      "{'word': 'asking', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'assisted', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'attendance', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'attitudes', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'attract', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'audience', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'authors', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'auto', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'autograph', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'available.this', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'avenue', 'docs': '111100011100', 'lastSeenIndex': 9}\n",
      "{'word': 'average', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'avoid', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'avoided', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'away', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'away.barget', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'baby', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'back', 'docs': '11110000110010101101', 'lastSeenIndex': 10}\n",
      "{'word': 'backbreaking', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'bad', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'bag', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'bags', 'docs': '11110000111011', 'lastSeenIndex': 8}\n",
      "{'word': 'baldwin', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'balls', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'ban', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'ban.city', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'band', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'bang', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'barco', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'barget', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'barney', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'bartering', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'basic', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'basis', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'batteries', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'bay', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'beach', 'docs': '11001101', 'lastSeenIndex': 5}\n",
      "{'word': 'become', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'behind', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'believes', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'beloved', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'berserk', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'best', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'better', 'docs': '11110001111001', 'lastSeenIndex': 9}\n",
      "{'word': 'bicycles', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'big', 'docs': '111100001011011101', 'lastSeenIndex': 9}\n",
      "{'word': 'bigger', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'bikers', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'bill', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'binoculars.', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'biopsy', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'birder', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'birding', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'births', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'bizarre', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'black', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'blanket', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'blind', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'block', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'blocks', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'blue', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'bob', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'book', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'books', 'docs': '111011111000', 'lastSeenIndex': 7}\n",
      "{'word': 'bookstore', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'boon', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'boots', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'born', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'bottle', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'bottles', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'bought', 'docs': '1011001100111001', 'lastSeenIndex': 5}\n",
      "{'word': 'bowling', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'boxes', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'boy', 'docs': '11110001111010', 'lastSeenIndex': 9}\n",
      "{'word': 'braked', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'brakes', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'brand', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'breakfast', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'bring', 'docs': '1100110111110001', 'lastSeenIndex': 5}\n",
      "{'word': 'brown', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'brush', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'bubble', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'budget', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'bulbs', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'bullet', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'bummer', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'burger', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'burn', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'burned', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'burst', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'bus', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'bus.jerry', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'buy', 'docs': '1011100011100111110010', 'lastSeenIndex': 5}\n",
      "{'word': 'caliber', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'california', 'docs': '1101111000', 'lastSeenIndex': 3}\n",
      "{'word': 'california.', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'called', 'docs': '11110011110010', 'lastSeenIndex': 14}\n",
      "{'word': 'came', 'docs': '101110101100111101001010', 'lastSeenIndex': 1}\n",
      "{'word': 'candy', 'docs': '11110000111001', 'lastSeenIndex': 8}\n",
      "{'word': 'cane', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'canine', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'cans', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'canton', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'captured', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'car', 'docs': '11101111001011100010', 'lastSeenIndex': 10}\n",
      "{'word': 'car.sam', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'careful', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'carpenter', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'carriages', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'cars', 'docs': '11001011101011110000', 'lastSeenIndex': 3}\n",
      "{'word': 'carson', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'carts', 'docs': '11110011111000', 'lastSeenIndex': 15}\n",
      "{'word': 'cat', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'cataract', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'cause', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'caused', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'causing', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'cell', 'docs': '111000111001', 'lastSeenIndex': 4}\n",
      "{'word': 'cells', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'cents', 'docs': '1101111011', 'lastSeenIndex': 3}\n",
      "{'word': 'certificates–one', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'change', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'change.sara', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'changed', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'changing', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'channels', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'charged', 'docs': '110011110001', 'lastSeenIndex': 2}\n",
      "{'word': 'chase', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'chase.they', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'cheap', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'cheaper', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'check', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'chef', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'childless', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'children', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'choose', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'church', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'cigarettes', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'cigars', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'cities', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'city', 'docs': '1100111001', 'lastSeenIndex': 2}\n",
      "{'word': 'clean', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'cleared', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'clearing', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'clicked', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'clients', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'clockwork', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'clone', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'clones', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'cloning', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'close', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'clothing', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'club', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'clubs.much', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'coffee', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'coffers', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'coin', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'collection', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'collision', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'color', 'docs': '1011110101', 'lastSeenIndex': 1}\n",
      "{'word': 'colors', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'come', 'docs': '11100111101011001010', 'lastSeenIndex': 5}\n",
      "{'word': 'comic', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'community', 'docs': '110011110101', 'lastSeenIndex': 2}\n",
      "{'word': 'company', 'docs': '111000111001', 'lastSeenIndex': 4}\n",
      "{'word': 'complain', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'complaints', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'completely', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'complex', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'complications', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'complied', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'concerned', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'condo', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'condominium', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'cone', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'cons', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'considerate', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'consideration', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'consisting', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'contestant', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'continued', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'conversations', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'convertible', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'correct', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'correctional', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'correctly', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'cost', 'docs': '11100011110000', 'lastSeenIndex': 4}\n",
      "{'word': 'could', 'docs': '1011110100', 'lastSeenIndex': 1}\n",
      "{'word': 'counts', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'county', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'couple', 'docs': '1011101011110100', 'lastSeenIndex': 1}\n",
      "{'word': 'course', 'docs': '110111110100', 'lastSeenIndex': 3}\n",
      "{'word': 'court', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'crazy', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'cream', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'created', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'creek', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'crew', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'crews', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'crossing', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'crosswalk', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'cub', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'cube', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'cultured', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'cup', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'currently', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'customers', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'customers.the', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'cut', 'docs': '11110000111000', 'lastSeenIndex': 8}\n",
      "{'word': 'cutting', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'cycles', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'daily', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'damage', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'day', 'docs': '1110011111000110', 'lastSeenIndex': 5}\n",
      "{'word': 'days', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'deal', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'deals', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'death', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'debating', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'debris', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'decide', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'decided', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'decision', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'decision.finally', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'delightful', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'delivered', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'delivers', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'demanding', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'demands', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'department', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'departments', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'desperate', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'destroyed', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'details', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'diabetes', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'diabetic', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'dictionaries', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'died', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'dies', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'different', 'docs': '111100111101', 'lastSeenIndex': 14}\n",
      "{'word': 'dining', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'directions', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'director', 'docs': '1011100011110010', 'lastSeenIndex': 5}\n",
      "{'word': 'dirt', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'discovered', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'disease', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'dogs', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'dollars', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'dolls', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'dom', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'dominic', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'donate', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'done', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'donna', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'donor.', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'door', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'door-slammer', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'doors', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'downtown', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'dozen', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'dreamily', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'drinks', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'drinks.people', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'drive', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'drive-through', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'driver', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'drivers', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'driving', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'drizzle', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'dropped', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'drought', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'drove', 'docs': '111010101101', 'lastSeenIndex': 10}\n",
      "{'word': 'early', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'earring', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'economy', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'eight', 'docs': '110110111100011100', 'lastSeenIndex': 4}\n",
      "{'word': 'either', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'elevation', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'elsewhere', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'embrace', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'emphysema', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'employee', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'empty', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'ended', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'ending', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'english', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'enough', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'environmental', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'erupted', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'estimated', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'estrus', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'ethnic', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'even', 'docs': '11001010101100111100001100', 'lastSeenIndex': 5}\n",
      "{'word': 'evening', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'event', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'ever', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'everett', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'every', 'docs': '110010111000111100101010', 'lastSeenIndex': 3}\n",
      "{'word': 'everyone', 'docs': '1110011100110111110010', 'lastSeenIndex': 5}\n",
      "{'word': 'exact', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'except', 'docs': '111011111000', 'lastSeenIndex': 7}\n",
      "{'word': 'exchange', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'excitedly', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'exhibitors', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'experimenting', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'explain', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'extra', 'docs': '110011110010', 'lastSeenIndex': 2}\n",
      "{'word': 'extra-large', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'eye', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'eyesight', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'facility', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'fact', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'failure', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'families', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'fantastic', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'fault', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'featured', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'federal', 'docs': '111100011101', 'lastSeenIndex': 9}\n",
      "{'word': 'fee', 'docs': '1100111001', 'lastSeenIndex': 2}\n",
      "{'word': 'feel', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'feet', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'felix', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'female', 'docs': '1110001100', 'lastSeenIndex': 4}\n",
      "{'word': 'festival', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'festivals', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'fiction', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'fight', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'fill', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'filled.no', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'finally', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'find', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'fine', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'fines', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'fire', 'docs': '11110011101101', 'lastSeenIndex': 15}\n",
      "{'word': 'first', 'docs': '111000101110001110111100', 'lastSeenIndex': 5}\n",
      "{'word': 'five', 'docs': '1110011110011110111100', 'lastSeenIndex': 5}\n",
      "{'word': 'five-year-old', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'fixing', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'flipped', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'following', 'docs': '10111010', 'lastSeenIndex': 1}\n",
      "{'word': 'food', 'docs': '11101110111001', 'lastSeenIndex': 8}\n",
      "{'word': 'foods', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'foot', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'foothill', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'forward', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'found', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'founders', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'four', 'docs': '1010111100101100', 'lastSeenIndex': 2}\n",
      "{'word': 'four-door', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'four-slice', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'francisco', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'free', 'docs': '11101010', 'lastSeenIndex': 7}\n",
      "{'word': 'freeway', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'fresh', 'docs': '111000110011110001', 'lastSeenIndex': 4}\n",
      "{'word': 'friction', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'friendly', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'friends', 'docs': '1110011111000010', 'lastSeenIndex': 5}\n",
      "{'word': 'front', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'fruit', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'full', 'docs': '1111001111001100', 'lastSeenIndex': 15}\n",
      "{'word': 'funds', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'furniture', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'future', 'docs': '111001111011', 'lastSeenIndex': 5}\n",
      "{'word': 'gallon', 'docs': '1101111011', 'lastSeenIndex': 3}\n",
      "{'word': 'game', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'gangster', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'gangsters', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'garbage', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'gas', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'gasoline', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'gave', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'gear', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'generating', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'get', 'docs': '110010110011001110111010', 'lastSeenIndex': 5}\n",
      "{'word': 'gets', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'getting', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'geyser', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'gift', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'give', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'given', 'docs': '11101010', 'lastSeenIndex': 7}\n",
      "{'word': 'gloves', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'go', 'docs': '1011001100101111000010', 'lastSeenIndex': 5}\n",
      "{'word': 'goal', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'goes', 'docs': '11100010110011110001', 'lastSeenIndex': 5}\n",
      "{'word': 'going', 'docs': '111100011101', 'lastSeenIndex': 9}\n",
      "{'word': 'golf', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'gone.', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'good', 'docs': '111011111001', 'lastSeenIndex': 7}\n",
      "{'word': 'got', 'docs': '1111000110101010', 'lastSeenIndex': 10}\n",
      "{'word': 'government', 'docs': '111001111011', 'lastSeenIndex': 5}\n",
      "{'word': 'grabbed', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'grand', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'great', 'docs': '111001110011110010', 'lastSeenIndex': 5}\n",
      "{'word': 'greater', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'grocery', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'groups—save', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'groups—the', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'grow', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'growing', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'guarantee', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'guards', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'guess', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'guessed', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'guessing', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'guides', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'gun', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'gunshots', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'guy', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'guy.jerry', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'half', 'docs': '111000110110111100011100', 'lastSeenIndex': 4}\n",
      "{'word': 'halloween', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'halt.turning', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'hamburgers', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'hamilton', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'handed', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'handle', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'happen', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'happened', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'happy', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'hauled', 'docs': '11110000111011', 'lastSeenIndex': 8}\n",
      "{'word': 'hawaiian', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'hazard', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'head', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'headaches', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'heads', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'heard', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'held', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'help', 'docs': '1111010010', 'lastSeenIndex': 13}\n",
      "{'word': 'herman', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'high', 'docs': '11001101111011', 'lastSeenIndex': 5}\n",
      "{'word': 'higher', 'docs': '11011100', 'lastSeenIndex': 5}\n",
      "{'word': 'hill', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'hire', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'hit', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'hold', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'holiday', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'home', 'docs': '101111000111110101', 'lastSeenIndex': 1}\n",
      "{'word': 'homebuyers', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'homeowner', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'homeowners', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'homes', 'docs': '110011110010', 'lastSeenIndex': 2}\n",
      "{'word': 'hope', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'hopes', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'horsetrail', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'hospital', 'docs': '111100011100', 'lastSeenIndex': 9}\n",
      "{'word': 'hot', 'docs': '111010111000', 'lastSeenIndex': 10}\n",
      "{'word': 'hotel', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'hour', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'hourly', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'hours', 'docs': '1111000111100110', 'lastSeenIndex': 9}\n",
      "{'word': 'house', 'docs': '1100110111110000', 'lastSeenIndex': 5}\n",
      "{'word': 'housing', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'howard', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'however', 'docs': '110011110001', 'lastSeenIndex': 2}\n",
      "{'word': 'huge', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'hurriedly', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'husband', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'hydrant', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'ice', 'docs': '11101111110000', 'lastSeenIndex': 7}\n",
      "{'word': 'idea', 'docs': '1110011100', 'lastSeenIndex': 5}\n",
      "{'word': 'ideas', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'immediately', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'implanted', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'inc.', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'income', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'incomes', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'inconsiderate', 'docs': '110011110100', 'lastSeenIndex': 2}\n",
      "{'word': 'inevitably', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'inferno', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'influence', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'injured', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'inmates', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'insurance', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'interest', 'docs': '11100111110000', 'lastSeenIndex': 5}\n",
      "{'word': 'invite', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'irritating', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'issued', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'item', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'items', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'jail', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'japanese–he', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'jerry', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'job', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'job-related', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'john', 'docs': '111010111000', 'lastSeenIndex': 10}\n",
      "{'word': 'johnson', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'joke', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'joy', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'jumped', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'kick', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'kids', 'docs': '111100101101', 'lastSeenIndex': 10}\n",
      "{'word': 'killing', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'kinds', 'docs': '111011111010', 'lastSeenIndex': 7}\n",
      "{'word': 'kitten', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'kittens', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'knee.jerry', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'knew', 'docs': '111011111011', 'lastSeenIndex': 7}\n",
      "{'word': 'know', 'docs': '11001101111011', 'lastSeenIndex': 5}\n",
      "{'word': 'lake', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'landfill', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'landlord', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'lane', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'lap', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'last', 'docs': '11100111110000', 'lastSeenIndex': 5}\n",
      "{'word': 'late', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'later', 'docs': '1110001010', 'lastSeenIndex': 5}\n",
      "{'word': 'latest', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'lawn', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'leading', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'least', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'lee', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'left', 'docs': '11110001111001', 'lastSeenIndex': 9}\n",
      "{'word': 'leftovers', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'legal', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'less', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'let', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'letting', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'libraries', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'life', 'docs': '111100011110001010', 'lastSeenIndex': 9}\n",
      "{'word': 'light', 'docs': '111101011100', 'lastSeenIndex': 15}\n",
      "{'word': 'like', 'docs': '1011100011011111000010', 'lastSeenIndex': 5}\n",
      "{'word': 'liked', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'limp', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'line', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'lined', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'lines', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'lips', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'liquid', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'listen', 'docs': '110011110100', 'lastSeenIndex': 2}\n",
      "{'word': 'listening', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'little', 'docs': '111001111000111011', 'lastSeenIndex': 5}\n",
      "{'word': 'lived', 'docs': '101111000111110100', 'lastSeenIndex': 1}\n",
      "{'word': 'lives', 'docs': '111000111000', 'lastSeenIndex': 4}\n",
      "{'word': 'living', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'loan', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'loans', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'local', 'docs': '11100111110000', 'lastSeenIndex': 5}\n",
      "{'word': 'long', 'docs': '11011100', 'lastSeenIndex': 5}\n",
      "{'word': 'look', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'looking', 'docs': '111001111011', 'lastSeenIndex': 5}\n",
      "{'word': 'looking.', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'los', 'docs': '111011111000', 'lastSeenIndex': 7}\n",
      "{'word': 'lose', 'docs': '11100111110000', 'lastSeenIndex': 5}\n",
      "{'word': 'lot', 'docs': '11001011110010', 'lastSeenIndex': 3}\n",
      "{'word': 'lottery', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'loud', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'love', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'loved', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'lowest', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'lozano', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'lucky', 'docs': '11110011101100', 'lastSeenIndex': 14}\n",
      "{'word': 'luxury', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'made', 'docs': '111001111001', 'lastSeenIndex': 5}\n",
      "{'word': 'magazine', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'magazines', 'docs': '11101110', 'lastSeenIndex': 8}\n",
      "{'word': 'main', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'maintenance', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'major', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'make', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'making', 'docs': '110010', 'lastSeenIndex': 3}\n",
      "{'word': 'man', 'docs': '1101110111110000110010', 'lastSeenIndex': 3}\n",
      "{'word': 'managed', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'manager', 'docs': '1010101101', 'lastSeenIndex': 3}\n",
      "{'word': 'many', 'docs': '111011111011', 'lastSeenIndex': 7}\n",
      "{'word': 'market', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'married', 'docs': '111100101101', 'lastSeenIndex': 10}\n",
      "{'word': 'massive', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'material', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'mcrap.the', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'mechanic', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'medical', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'meeting', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'messages', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'middle-aged', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'might', 'docs': '110011100011110101', 'lastSeenIndex': 2}\n",
      "{'word': 'mild', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'mile', 'docs': '1011110001', 'lastSeenIndex': 1}\n",
      "{'word': 'miles', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'milk', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'milkplus', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'million', 'docs': '111101001101', 'lastSeenIndex': 15}\n",
      "{'word': 'millions', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'mind', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'minimum', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'minor', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'minutes', 'docs': '101100', 'lastSeenIndex': 3}\n",
      "{'word': 'money', 'docs': '101011110010', 'lastSeenIndex': 2}\n",
      "{'word': 'money.', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'money.sam', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'monica', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'month', 'docs': '11110001111000', 'lastSeenIndex': 9}\n",
      "{'word': 'months', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'more.', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'morning', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'mountain', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'move', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'moved', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'movie', 'docs': '10111010', 'lastSeenIndex': 1}\n",
      "{'word': 'movies', 'docs': '10111010', 'lastSeenIndex': 1}\n",
      "{'word': 'mr.', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'mrs', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'mrs.', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'much', 'docs': '1101111010', 'lastSeenIndex': 3}\n",
      "{'word': 'murder-suicide', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'music', 'docs': '110011110100', 'lastSeenIndex': 2}\n",
      "{'word': 'must', 'docs': '11100010111000111011', 'lastSeenIndex': 5}\n",
      "{'word': 'n.', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'name', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'names', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'nancy', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'nationwide', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'near', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'nearby', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'necessary', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'necessary.', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'negotiations', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'neighbor', 'docs': '1111010110', 'lastSeenIndex': 14}\n",
      "{'word': 'neighborhood', 'docs': '110111110010', 'lastSeenIndex': 3}\n",
      "{'word': 'neighbors', 'docs': '110011110100', 'lastSeenIndex': 2}\n",
      "{'word': 'never', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'nevertheless', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'new', 'docs': '101101101011110001', 'lastSeenIndex': 5}\n",
      "{'word': 'newest', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'newspaper', 'docs': '111011111000', 'lastSeenIndex': 7}\n",
      "{'word': 'newsstand', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'next', 'docs': '110011110010101100', 'lastSeenIndex': 2}\n",
      "{'word': 'next-door', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'nice', 'docs': '1011011011110000', 'lastSeenIndex': 5}\n",
      "{'word': 'nicest', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'night', 'docs': '110011110011', 'lastSeenIndex': 2}\n",
      "{'word': 'night—', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'nine', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'nitrogen', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'nobody', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'noise', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'nonfat', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'nonflammable', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'north', 'docs': '1011110010', 'lastSeenIndex': 1}\n",
      "{'word': 'northville', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'notify', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'number', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'numbers', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'nutritious', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'occurred', 'docs': '1110111110101100', 'lastSeenIndex': 7}\n",
      "{'word': 'occurs', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'oceanside', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'octane', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'offer', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'offers', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'officer', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'officers', 'docs': '111100001101', 'lastSeenIndex': 8}\n",
      "{'word': 'official', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'officials', 'docs': '110011011101111011', 'lastSeenIndex': 5}\n",
      "{'word': 'offset', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'often', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'oil', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'ok', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'old', 'docs': '101111000010', 'lastSeenIndex': 1}\n",
      "{'word': 'one', 'docs': '101011011010110011101010101010', 'lastSeenIndex': 5}\n",
      "{'word': 'one-bedroom', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'one-hour', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'ones', 'docs': '1011110101', 'lastSeenIndex': 1}\n",
      "{'word': 'onto', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'operation.ninety', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'opportunity.', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'order', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'ordering', 'docs': '111010111001', 'lastSeenIndex': 6}\n",
      "{'word': 'others', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'ought', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'outdoor', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'outdoors', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'outside', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'overpaid', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'overpaying', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'owner', 'docs': '11001101111000', 'lastSeenIndex': 5}\n",
      "{'word': 'owners', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'owns', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'p.m.', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'paid', 'docs': '111001111001111011', 'lastSeenIndex': 5}\n",
      "{'word': 'pain.', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'paper', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'paper.the', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'park', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'parked', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'parking', 'docs': '1100111001', 'lastSeenIndex': 2}\n",
      "{'word': 'part', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'party', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'pasadena', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'passing', 'docs': '11011100', 'lastSeenIndex': 5}\n",
      "{'word': 'pay', 'docs': '11100010111000111011', 'lastSeenIndex': 5}\n",
      "{'word': 'paying', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'penny', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'people', 'docs': '110010111000111100011010', 'lastSeenIndex': 3}\n",
      "{'word': 'per', 'docs': '110010', 'lastSeenIndex': 3}\n",
      "{'word': 'percent', 'docs': '11001011001100101110111100', 'lastSeenIndex': 5}\n",
      "{'word': 'perfect', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'period', 'docs': '1110111100', 'lastSeenIndex': 9}\n",
      "{'word': 'permitted', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'personal', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'persons', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'phoenix', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'phone', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'pianist', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'piano', 'docs': '11110001111001', 'lastSeenIndex': 9}\n",
      "{'word': 'picked', 'docs': '11110001111001', 'lastSeenIndex': 9}\n",
      "{'word': 'pile', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'pine', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'pistol', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'pizza', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'place', 'docs': '1011100011110000', 'lastSeenIndex': 5}\n",
      "{'word': 'planners', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'plants', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'plastic', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'plates', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'play', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'played', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'player', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'playground.', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'playing', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'plot', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'plowed', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'plus', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'police', 'docs': '1100101111000011001010', 'lastSeenIndex': 3}\n",
      "{'word': 'popular', 'docs': '11101110', 'lastSeenIndex': 8}\n",
      "{'word': 'portable', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'post', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'practice', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'practiced', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'president', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'pretty', 'docs': '110111110001', 'lastSeenIndex': 3}\n",
      "{'word': 'price', 'docs': '11011010111001', 'lastSeenIndex': 5}\n",
      "{'word': 'prices', 'docs': '110111001100111011', 'lastSeenIndex': 5}\n",
      "{'word': 'prince', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'prison', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'prisoners', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'private', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'probably', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'problem', 'docs': '1100111000', 'lastSeenIndex': 2}\n",
      "{'word': 'problems', 'docs': '110011110010', 'lastSeenIndex': 2}\n",
      "{'word': 'proceeds', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'produce', 'docs': '11001100', 'lastSeenIndex': 4}\n",
      "{'word': 'profit', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'property', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'proposal', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'proprietor', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'pros', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'proven', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'provide', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'pulp', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'pumping', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'puppies', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'pursuing', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'put', 'docs': '1111001110', 'lastSeenIndex': 12}\n",
      "{'word': 'quell', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'question', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'question-and-answer', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'quieter', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'quieter.', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'quite', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'radios', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'raffle', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'raging', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'rain', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'raining', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'rainstorm', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'raises', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'ran', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'range', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'rates', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'rather', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'ration', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'rattle', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'razors', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'reading', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'reads', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'ready', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'realtor', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'reason', 'docs': '110111110011', 'lastSeenIndex': 3}\n",
      "{'word': 'receives', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'recent', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'reconsider', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'reduce', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'reduced', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'reduction', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'refill', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'refused', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'regular', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'released', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'remaining', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'remarked', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'remove', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'removed', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'rent', 'docs': '110011110100', 'lastSeenIndex': 2}\n",
      "{'word': 'rental', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'rentals', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'renters', 'docs': '110011110100', 'lastSeenIndex': 2}\n",
      "{'word': 'renting', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'rents', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'repairs', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'resident', 'docs': '111100101100', 'lastSeenIndex': 10}\n",
      "{'word': 'residents', 'docs': '11001011100011110001', 'lastSeenIndex': 3}\n",
      "{'word': 'respond', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'response', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'rest', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'restaurant', 'docs': '10111001', 'lastSeenIndex': 1}\n",
      "{'word': 'restored', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'resulted', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'resumed', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'retired', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'retirees', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'return', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'revolver', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'rick', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'right', 'docs': '101100111010', 'lastSeenIndex': 3}\n",
      "{'word': 'road', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'roadside', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'roof', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'room', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'ruined', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'run', 'docs': '11011100111000111010', 'lastSeenIndex': 5}\n",
      "{'word': 'sad', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'safely', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'said', 'docs': '110010101010101010111011101010', 'lastSeenIndex': 5}\n",
      "{'word': 'sale', 'docs': '111001111001', 'lastSeenIndex': 5}\n",
      "{'word': 'sam', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'samantha', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'san', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'sandwich', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'sandwiches', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'santa', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'sara', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'saturday', 'docs': '10111001101111010110', 'lastSeenIndex': 1}\n",
      "{'word': 'save', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'saved', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'savings', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'saw', 'docs': '11100111110001', 'lastSeenIndex': 5}\n",
      "{'word': 'saxophone', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'saxophonist', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'say', 'docs': '1011100011101110', 'lastSeenIndex': 5}\n",
      "{'word': 'saying', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'says', 'docs': '11100010111000', 'lastSeenIndex': 5}\n",
      "{'word': 'scalding.he', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'scared', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'scheduled', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'scouts', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'seashell', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'seatbelt', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'second', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'secured', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'security', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'seekers', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'seem', 'docs': '11100011110010', 'lastSeenIndex': 4}\n",
      "{'word': 'seemed', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'selected', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'sell', 'docs': '11100111110010', 'lastSeenIndex': 5}\n",
      "{'word': 'seller', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'sent', 'docs': '111000111000', 'lastSeenIndex': 4}\n",
      "{'word': 'set', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'seven', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'several', 'docs': '110111110000', 'lastSeenIndex': 3}\n",
      "{'word': 'shaking', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'shapes', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'shave', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'sherman', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'shiny', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'shopping', 'docs': '11110010111001', 'lastSeenIndex': 10}\n",
      "{'word': 'shorter', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'show', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'sicker', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'sickness', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'side', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'sidewalks.noise', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'sign', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'silverware', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'since', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'six', 'docs': '111100011101', 'lastSeenIndex': 9}\n",
      "{'word': 'sixth', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'sixty-year-old', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'sizes', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'skyrocketing', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'slacks', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'slam', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'sleeping', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'slightly', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'slowly', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'slumped', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'smiled', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'smith', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'sneaking', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'soap', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'social', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'sofas', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'sold', 'docs': '1110011100', 'lastSeenIndex': 5}\n",
      "{'word': 'something', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'sometime', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'sometimes', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'sought', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'southern', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'southland', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'space', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'spanish', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'sparing', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'specter', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'spewed', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'spill', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'spilled', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'spinning', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'sponsored', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'sports', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'spring', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'stain', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'stand', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'standing', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'stars', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'started', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'state', 'docs': '111001110110111011', 'lastSeenIndex': 5}\n",
      "{'word': 'station', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'stationery', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'stay', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'steadily', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'steep', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'steering', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'still', 'docs': '1100110111100010', 'lastSeenIndex': 5}\n",
      "{'word': 'stolen', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'stop', 'docs': '111100011100', 'lastSeenIndex': 9}\n",
      "{'word': 'stopped', 'docs': '110011110000', 'lastSeenIndex': 2}\n",
      "{'word': 'store', 'docs': '101100111011', 'lastSeenIndex': 3}\n",
      "{'word': 'stored', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'stray', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'streambed', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'street', 'docs': '111100011010', 'lastSeenIndex': 10}\n",
      "{'word': 'streets', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'stretch', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'studio', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'succeed', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'successfully', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'sudden', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'sue', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'sued', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'suffered', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'suggested', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'summer', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'sunday', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'supposed', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'surcharge', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'sure', 'docs': '110011110010', 'lastSeenIndex': 2}\n",
      "{'word': 'surround', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'surrounded', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'survive', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'suv', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'table', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'take', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'taken', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'talk', 'docs': '111011111010', 'lastSeenIndex': 7}\n",
      "{'word': 'talked', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'talking', 'docs': '11110001111000', 'lastSeenIndex': 9}\n",
      "{'word': 'talks', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'tank', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'tax', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'taxes', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'teen', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'teenage', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'teenager', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'tell', 'docs': '10110011110010', 'lastSeenIndex': 3}\n",
      "{'word': 'telling', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'theater', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'thelma', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'them.some', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'thick', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'thing', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'things', 'docs': '1110011110101100', 'lastSeenIndex': 5}\n",
      "{'word': 'think', 'docs': '110011110010', 'lastSeenIndex': 2}\n",
      "{'word': 'thinks', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'though', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'thought', 'docs': '11100111110010', 'lastSeenIndex': 5}\n",
      "{'word': 'thousands', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'three', 'docs': '10101111000011110001111000', 'lastSeenIndex': 2}\n",
      "{'word': 'three-bedroom', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'three-hour', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'threw', 'docs': '111100001101', 'lastSeenIndex': 8}\n",
      "{'word': 'tim', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'time', 'docs': '101101111100011100', 'lastSeenIndex': 4}\n",
      "{'word': 'time.', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'times', 'docs': '101100111010', 'lastSeenIndex': 3}\n",
      "{'word': 'tired', 'docs': '11100111110001', 'lastSeenIndex': 5}\n",
      "{'word': 'tires', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'toaster', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'today', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'together', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'toilet', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'told', 'docs': '11100111110001', 'lastSeenIndex': 5}\n",
      "{'word': 'tons', 'docs': '11110100101100', 'lastSeenIndex': 15}\n",
      "{'word': 'too.', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'took', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'toppled', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'toss', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'totally', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'tough', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'toward', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'tower', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'town', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'toyola', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'trade', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'traffic', 'docs': '110111100011110001', 'lastSeenIndex': 3}\n",
      "{'word': 'trash', 'docs': '110011110101', 'lastSeenIndex': 2}\n",
      "{'word': 'tree', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'trees', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'tried', 'docs': '111100001101', 'lastSeenIndex': 8}\n",
      "{'word': 'triple-scoop', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'trucks', 'docs': '111101001101', 'lastSeenIndex': 15}\n",
      "{'word': 'try', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'trying', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'tune-ups', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'tuner', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'turned', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'tv', 'docs': '1110111100111010', 'lastSeenIndex': 9}\n",
      "{'word': 'twenty', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'twice', 'docs': '1110001111000010', 'lastSeenIndex': 4}\n",
      "{'word': 'twin', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'two', 'docs': '1100101101110010101111000010101010', 'lastSeenIndex': 3}\n",
      "{'word': 'two-', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'two-bedroom', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'two-mile', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'underbrush', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'unemployed', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'uneventful', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'unfortunately', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'unhappy', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'uninjured', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'unlucky', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'unpredictable', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'upset', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'us', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'use', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'used', 'docs': '10110011110010', 'lastSeenIndex': 3}\n",
      "{'word': 'value', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'variable', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'variables', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'vehicle.when', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'vehicles', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'vendor', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'venice', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'veterinarian', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'vicky', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'victoria', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'video', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'videos', 'docs': '10111010', 'lastSeenIndex': 1}\n",
      "{'word': 'visiting', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'visitors', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'vittorio', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'vittorios', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'volunteers', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'wait', 'docs': '1101101101', 'lastSeenIndex': 4}\n",
      "{'word': 'waited', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'waiting', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'walked', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'walks', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'want', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'warden', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'wash', 'docs': '111010', 'lastSeenIndex': 6}\n",
      "{'word': 'washes', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'watch', 'docs': '1011110110', 'lastSeenIndex': 1}\n",
      "{'word': 'watches', 'docs': '111011', 'lastSeenIndex': 7}\n",
      "{'word': 'watching', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'water', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'way', 'docs': '111100101100', 'lastSeenIndex': 10}\n",
      "{'word': 'weaned', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'wearing', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'website', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'week', 'docs': '110011101011110011', 'lastSeenIndex': 2}\n",
      "{'word': 'weekend', 'docs': '111011111011', 'lastSeenIndex': 7}\n",
      "{'word': 'weekly', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'weeks', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'well', 'docs': '101101111100001101', 'lastSeenIndex': 4}\n",
      "{'word': 'went', 'docs': '10111011110011110010', 'lastSeenIndex': 1}\n",
      "{'word': 'west', 'docs': '111011111000', 'lastSeenIndex': 7}\n",
      "{'word': 'whales', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'whatever', 'docs': '111001', 'lastSeenIndex': 5}\n",
      "{'word': 'wheel', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'wheels', 'docs': '1101', 'lastSeenIndex': 3}\n",
      "{'word': 'whether', 'docs': '11001101', 'lastSeenIndex': 5}\n",
      "{'word': 'whiskey', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'white', 'docs': '10', 'lastSeenIndex': 1}\n",
      "{'word': 'whole', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'whose', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'widow', 'docs': '11110100', 'lastSeenIndex': 12}\n",
      "{'word': 'widowed', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'wife', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'win', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'windows', 'docs': '11110000', 'lastSeenIndex': 8}\n",
      "{'word': 'winnings', 'docs': '11110001', 'lastSeenIndex': 9}\n",
      "{'word': 'woke', 'docs': '11110010', 'lastSeenIndex': 10}\n",
      "{'word': 'woman', 'docs': '11110101', 'lastSeenIndex': 13}\n",
      "{'word': 'work', 'docs': '101010111100011101', 'lastSeenIndex': 3}\n",
      "{'word': 'worked', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'world', 'docs': '1111010110', 'lastSeenIndex': 14}\n",
      "{'word': 'worms', 'docs': '1100', 'lastSeenIndex': 2}\n",
      "{'word': 'worth', 'docs': '1100110111100011110010', 'lastSeenIndex': 5}\n",
      "{'word': 'would', 'docs': '1010111000101010111100111010', 'lastSeenIndex': 2}\n",
      "{'word': 'wrong', 'docs': '11100111110001', 'lastSeenIndex': 5}\n",
      "{'word': 'year', 'docs': '1110001011001111000010', 'lastSeenIndex': 5}\n",
      "{'word': 'year.', 'docs': '111000', 'lastSeenIndex': 4}\n",
      "{'word': 'years', 'docs': '10101011001100110010111011101100', 'lastSeenIndex': 5}\n",
      "{'word': 'yelling', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'yellow', 'docs': '11110111', 'lastSeenIndex': 15}\n",
      "{'word': 'yet', 'docs': '11110110', 'lastSeenIndex': 14}\n",
      "{'word': 'yield', 'docs': '11110011', 'lastSeenIndex': 11}\n",
      "{'word': 'young', 'docs': '11011101', 'lastSeenIndex': 3}\n",
      "{'word': '’', 'docs': '101010101010101010111011101010', 'lastSeenIndex': 5}\n",
      "{'word': '“', 'docs': '10101010101100111011101010', 'lastSeenIndex': 5}\n",
      "{'word': '”', 'docs': '10101010101100111011101010', 'lastSeenIndex': 5}\n"
     ]
    }
   ],
   "source": [
    "test_directory.printGamaCodePostingList()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
