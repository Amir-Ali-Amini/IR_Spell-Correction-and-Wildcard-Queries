{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an Information Retrieval System with Spelling Correction and Wildcard Queries\n",
    "\n",
    "## AmirAli Amini - 610399102\n",
    "\n",
    "#### HW2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# توضیحات مسئله و چالش ها و بهبود ها\n",
    "\n",
    "بزرگ ترین چالش این مسئله ساخت درخت ترای بود که با ساختن دو کلاس یکی برای هر نود و یکی برای کل درخت انجام شد\n",
    "\n",
    "در این سوال تمام جایگشت های دوری یک رشته را با $ در انتهای آن به درست اضافه میکنم که بتوان طبق الگوریتم گفته شده در کتاب وایدکارت ها را بدست اورد؛\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## کتابخانه ها \n",
    "\n",
    "###  from nltk import word_tokenize :\n",
    "از این کتابخانه برای تکنایز کردن داده ها به این دلیل که توکنایز کردن دیتا سریع تر میشه استفاده کردم\n",
    "\n",
    "###  from nltk.corpus import stopwords :\n",
    "از این کتابخانه برای دریافت استاپینگ ورد های زبان انگلیسی استفاده کردم\n",
    "\n",
    "###  import string:\n",
    "از این کتابخانه برای دریافت پانچویشن های زبان انگلیسی استفاده کردم\n",
    "\n",
    "###  import numpy as np:\n",
    "از این کتابخانه برای جمع یک عدد با تمام اعضای یک آرایه استفاده کردم\n",
    "\n",
    "###  import copy:\n",
    "از این کتابخانه برای دیپ کپی کردن ارایه استفاده کردم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 61] Connection\n",
      "[nltk_data]     refused>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 61]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords # a library to tokenize input texts\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') # stopping word in English language\n",
    "\n",
    "import string # using to remove punctuation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "در این سوال فقط به توضیح توابع اضافه شده نسبت به تمرین سری اول میپردازم\n",
    "\n",
    "### node:\n",
    "این کلاس به صورت مشخص ایمپلیمنت کننده هر برگ درخت ترای است\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node : #class node for nodes of try tree\n",
    "    def __init__(self,isWord=False, myChar=None , myWord=None):\n",
    "        self.myChar = myChar\n",
    "        self.myWord = myWord\n",
    "        self.nodes = {} # children\n",
    "        self.seenChars = []  # chars in children\n",
    "        self.isAWrod = isWord # boolean to check if this node indicate a valid word or not\n",
    "    \n",
    "    def addNode(self ,char , isWord = False):  # add a child to self and return new node\n",
    "        if (char not in self.seenChars):\n",
    "            newNode = node(myChar=char , myWord=self.myWord+char if self.myWord else char , isWord=isWord)\n",
    "            self.nodes[char] = newNode\n",
    "            self.seenChars.append(char)\n",
    "            return newNode\n",
    "        if isWord:\n",
    "            self.nodes[char].makeWord()\n",
    "        return self.nodes[char]\n",
    "        \n",
    "    def makeWord(self): # set self node to a valid word\n",
    "        self.isAWrod=True\n",
    "\n",
    "    def getWord (self):\n",
    "        return self.myWord\n",
    "    \n",
    "    def getIsWord (self):\n",
    "        return self.isAWrod\n",
    "\n",
    "    def getChar (self):\n",
    "        return self.myChar\n",
    "    \n",
    "    def includesChar(self, char):\n",
    "        return char in self.seenChars\n",
    "    \n",
    "    def getSeenNodes (self):\n",
    "        return [self.nodes[char] for char in self.seenChars]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tryTree\n",
    "این کلاس هم درخت تری را ایمپلیمنت میکند و توضیح توابع در کامنت ها نوشته شده است\n",
    "\n",
    "تنها نکته مهم استفاده از \n",
    "BFS \n",
    "برای پیمایش زیر درخت هر نود است که یک نود را میگیرد و زیر درختش را پیمایش میکند و همه نود ها را برمیگرداند\n",
    "\n",
    "\n",
    "\n",
    "#### getBFSWords:\n",
    "\n",
    "این تابع کلمات ولید در نود های برگردانده شده از bfs را برمیگرداند\n",
    "\n",
    "#### BFSPrefix: \n",
    "تابع بی اف اس پریفیکس هم یک پیشوند برای تمام کلماتی که در درخت میخواهیم پیدا کنیم ورودی میگیرد سپس به نود ان پیشوند رفته و بی اف اس را از آن نود شروع میکند\n",
    "\n",
    "\n",
    "#### find:\n",
    "تابع فاند عملیات پیدا کردن وایلد کارت را رو درخت طبق الگوریتم گفته شده انجام میدهذ به این صورت که پریفیکس کلمه (مقدار قبل از ستاره) را در درخت جست و جو میکند سپس کلمات بازگشت داده شده را میچرخاند با علامت دلار در اخر آنها قرار گیرد.\n",
    "\n",
    "برای دو ستاره نیز برای یک ستاره این کار را انجام میدهد و برای ستاره دوم بین تمام کلمات بازگشتی آنهایی که دارای مقدار بین دو ستاره هستند را برمیگرداند \n",
    "\n",
    "توجه شود که علارت به صورتی میچرخد که یک ستاره در انتها و بین ستاره ها کمترین حروف قرار گیرد و اینکار برای افزایش سرعت سرچ میباشد\n",
    "\n",
    "همچنین در ابتدا تابع سرچ وایلد کارت به انتها ورودی کی علامت دلار اضافه میشود که نشان از انتها کلمه است.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class tryTree :\n",
    "    def __init__ (self):\n",
    "        self.root = node()\n",
    "\n",
    "    def insertWord(self, word): # insert a word to tree by adding all its chars respectively\n",
    "        currentNode  = self.root\n",
    "        for char in range(len(word)): \n",
    "            currentNode = currentNode.addNode(word[char] , isWord = char == len(word)-1)\n",
    "\n",
    "\n",
    "    def insertWordPermutation (self,inputWord): # insert all permutation of a word to the tree by adding $ to end of it\n",
    "        word = inputWord+\"$\"\n",
    "        for index in range(len(word)):\n",
    "            self.insertWord(word[index:]+word[:index])\n",
    "\n",
    "    def BFS(self , node): # BFS traversal of a subtree\n",
    "        result = [node]\n",
    "        for child in node.getSeenNodes():\n",
    "            result+= self.BFS(child)\n",
    "        return result\n",
    "    \n",
    "    def getBFSWords (self , node): # get valid word from result of BFS\n",
    "        ls = self.BFS(node)\n",
    "        return [node.getWord() for node in ls if node.getIsWord()]\n",
    "    \n",
    "    def prefixBFS (self, prefix= ''): # find prefix node and start BFS from it\n",
    "        currentNode = self.root\n",
    "        for char in range(len(prefix)): \n",
    "            currentNode = currentNode.addNode(prefix[char])\n",
    "        ls = self.getBFSWords(currentNode)\n",
    "        return ls\n",
    "\n",
    "    \n",
    "    def find(self, inputWord): # find wild cards in try tree\n",
    "        word = inputWord\n",
    "        if ('*' not in word):\n",
    "            return []\n",
    "        # if inputWord[-1] != \"*\":\n",
    "        word +='$'\n",
    "        startCount = word.count('*')\n",
    "        if (startCount ==1):\n",
    "            index = word.find('*')\n",
    "            prefix = word[index+1:]+word[:index]\n",
    "            ls = self.prefixBFS(prefix=prefix)\n",
    "            st = set()\n",
    "            for w in ls :\n",
    "                index = w.find('$')\n",
    "                st.add(w[index+1:]+w[:index])\n",
    "            return list(st)\n",
    "        elif(startCount==2):\n",
    "            while (word[-1]!='*'):\n",
    "                word = word[-1:]+word[:-1]\n",
    "            splitWord = word[:-1].split('*')\n",
    "            if len(splitWord[0])< len(splitWord[1]):\n",
    "                splitWord[0], splitWord[1] = splitWord[1],splitWord[0]\n",
    "            \n",
    "            ls = [i for i in self.prefixBFS(prefix=splitWord[0]) if splitWord[1] in i[len(splitWord[0]):]]\n",
    "\n",
    "            st = set()\n",
    "            for word in ls :\n",
    "                index = word.find('$')\n",
    "                st.add(word[index+1:]+word[:index])\n",
    "            return list(st)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def prnt (self,node=None):\n",
    "        root = self.root\n",
    "        if (node):\n",
    "            root = node\n",
    "        print (self.getBFSWords(root) )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class searchEngine:\n",
    "    def __init__(self , debug = False) -> None: # constructor of class\n",
    "        self.debug = debug\n",
    "        self.postingList =[]\n",
    "        self.files=[] \n",
    "        self.stop = set(stopwords.words('english') + list(string.punctuation)) # all extra expression which should ignore\n",
    "        self.tryTree = tryTree()\n",
    "\n",
    "        # structure of postingList : list of {word : nameOfWord , docs :[list of {doc:nameOfDocument , indexes: indexes of the word if this document}]}\n",
    "\n",
    "\n",
    "    # binary search to find a word in posting list\n",
    "    def searchPostingList(self, word):\n",
    "        s= 0 \n",
    "        e = len(self.postingList)\n",
    "        if e <=0 :\n",
    "            return 0\n",
    "        e-=1\n",
    "        while (1):\n",
    "            if (e-s < 2):\n",
    "                if (self.postingList[e][\"word\"] < word):\n",
    "                    return e+1\n",
    "                if (self.postingList[e][\"word\"] == word):\n",
    "                    return e\n",
    "                if (self.postingList[s][\"word\"] >= word):\n",
    "                    return s\n",
    "\n",
    "                return e\n",
    "            mid = (s+e)/2\n",
    "            mid = int(mid)\n",
    "            if (word<self.postingList[mid][\"word\"]):\n",
    "                e=mid\n",
    "            elif (word> self.postingList[mid][\"word\"]):\n",
    "                s = mid\n",
    "            else :\n",
    "                return mid\n",
    "            \n",
    "        # {word:str, indexes:list(int)}\n",
    "    # binary search to find a word in each dictionary\n",
    "    def searchDictionary(self, word,ls):\n",
    "        s= 0 \n",
    "        e = len(ls)\n",
    "        if e <=0 :\n",
    "            return 0\n",
    "        e-=1\n",
    "        while (1):\n",
    "            if (e-s < 2):\n",
    "                if (ls[e][\"doc\"] < word):\n",
    "                    return e+1\n",
    "                if (ls[e][\"doc\"] == word):\n",
    "                    return e\n",
    "                if (ls[s][\"doc\"] >= word):\n",
    "                    return s\n",
    "\n",
    "                return e\n",
    "            mid = (s+e)/2\n",
    "            mid = int(mid)\n",
    "            if (word<ls[mid][\"doc\"]):\n",
    "                e=mid\n",
    "            elif (word> ls[mid][\"doc\"]):\n",
    "                s = mid\n",
    "            else :\n",
    "                return mid\n",
    "\n",
    "\n",
    "            \n",
    "        # {doc:number, indexes:list(int)}\n",
    "\n",
    "\n",
    "\n",
    "    def addToPostingList(self, tokenizedText: list[str],docIndex:int): # add tokenized word in posting list \n",
    "        for i in range(len(tokenizedText)):\n",
    "            word = tokenizedText[i]\n",
    "            index = self.searchPostingList(word) # find index of word in posting list\n",
    "            if (len(self.postingList)>index): # check if index is not larger than posting list (if word is bigger that all words, search function returns len(postingList)+1)\n",
    "                if (self.postingList[index][\"word\"] == word): # check if index is the index of the word\n",
    "                    if (self.postingList[index][\"docs\"][-1][\"doc\"] == docIndex): # if we have already added the document index \n",
    "                        self.postingList[index][\"docs\"][-1][\"indexes\"].append(i) # as we read tokens in order of their index, we need to add token in end of the list\n",
    "\n",
    "                    else:\n",
    "                        self.postingList[index][\"docs\"].append({\"doc\":docIndex,\"indexes\":[i] }) # if we have not already added the document and dou to the fact that they are read in order of their index, we can easily add append new one in end of the list \n",
    "\n",
    "                else :\n",
    "                    self.postingList[index:index]= [({\"word\":word , \"docs\":[{\"doc\":docIndex , \"indexes\":[i]}]})] # word is bigger that all other words => we can append it to end of the list\n",
    "\n",
    "            else :\n",
    "                self.postingList.append({\"word\":word , \"docs\":[{\"doc\":docIndex , \"indexes\":[i]}]}) # we have not already added the word and dou to the fact that they are read in order of their index, we can easily add append new one in end of the list\n",
    "\n",
    "            self.tryTree.insertWordPermutation(word)\n",
    "\n",
    "\n",
    "    def input (self, filePath: list[str]): # input paths of inputs\n",
    "        for i in range(len(filePath)): # for files in input\n",
    "            file = open(filePath[i],'r',encoding='cp1252') # open the file\n",
    "            text = file.read() # read the file\n",
    "            file.close()  # close the file\n",
    "            # tokenize text and ignore stopping words using nltk library \n",
    "            tokenizedText = [word for word in word_tokenize(text.lower(),preserve_line=False) if word not in self.stop] \n",
    "            print (f'document {i+1} : {filePath[i]}')\n",
    "            # print(tokenizedText)\n",
    "            self.addToPostingList(tokenizedText , i+1) # i indicates to index of document we are reading\n",
    "        \n",
    "\n",
    "\n",
    "    def findWord(self, word): # this function use our binary search function to find word in posting list and if the word is not included in the list, returns -1\n",
    "        index = self.searchPostingList(word)\n",
    "        if (index< len(self.postingList)):\n",
    "            if (self.postingList[index][\"word\"] == word):\n",
    "                return index # real index of the word\n",
    "        return -1 # word is not in the posting list\n",
    "\n",
    "\n",
    "    def find(self , query:str): # split the query and find the result \n",
    "        splitQuery = query.lower().split()\n",
    "        if (len(splitQuery)==1): # query is only one word\n",
    "            index = self.findWord(splitQuery[0])\n",
    "            if (index>-1):\n",
    "                return [ i[\"doc\"] for i in self.postingList[index][\"docs\"]]\n",
    "            return []\n",
    "\n",
    "        else: \n",
    "            index1 = self.findWord(splitQuery[0])\n",
    "            index2 = self.findWord(splitQuery[2])\n",
    "            if splitQuery[1] in [\"and\" ,\"or\", \"AND\", \"OR\"]: # boolean condition\n",
    "                \n",
    "                if (splitQuery[1] in [\"and\",\"AND\"]): # and condition\n",
    "                    if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                        docs1 =  set([ i[\"doc\"] for i in self.postingList[index1][\"docs\"]]) # find document of word one \n",
    "                        docs2 =  set([ i[\"doc\"] for i in self.postingList[index2][\"docs\"]]) # find document of word two \n",
    "                        # print(self.postingList[index1][\"docs\"] , self.postingList[index2][\"docs\"])\n",
    "                        return list(docs1.intersection(docs2)) # make intersection of two results\n",
    "                    return []\n",
    "                    \n",
    "                if (splitQuery[1] in [\"or\",\"OR\"]): # or condition\n",
    "                    if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                        docs1 =  set([ i[\"doc\"] for i in self.postingList[index1][\"docs\"]])\n",
    "                        docs2 =  set([ i[\"doc\"] for i in self.postingList[index2][\"docs\"]])\n",
    "                        # print(self.postingList[index1][\"docs\"] , self.postingList[index2][\"docs\"])\n",
    "                        return list(docs1.union(docs2))\n",
    "                    if(index1!=-1): return [ i[\"doc\"] for i in self.postingList[index1][\"docs\"]] # check if there is result for first word\n",
    "                    return [ i[\"doc\"] for i in self.postingList[index2][\"docs\"]] # check if there is result for second word\n",
    "\n",
    "            else : # near condition\n",
    "                nearNumber = int(splitQuery[1].split('/')[1]) # find near \n",
    "                if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                    docs1 =   {i[\"doc\"]:i[\"indexes\"] for i in self.postingList[index1][\"docs\"]} # find document of word one and make dictionary for result\n",
    "                    docs2 =   {i[\"doc\"]:i[\"indexes\"] for i in self.postingList[index2][\"docs\"]} # find document of word two and make dictionary for result\n",
    "\n",
    "                    result = [] \n",
    "                    keysOfDocs2 = docs2.keys()\n",
    "                    for key, value in docs1.items():\n",
    "                        if (key in keysOfDocs2):\n",
    "                            d1 = np.array(value) # indexes of word in first document\n",
    "                            d2 = np.array(copy.deepcopy(docs2[key])) # indexes of word in second document\n",
    "                            distanceMatrix = np.array([[abs(i - j )for i in d1] for j in d2]) # distance matrix\n",
    "                            if (distanceMatrix.min() <= nearNumber): # check validation\n",
    "                                result.append(key)\n",
    "\n",
    "\n",
    "                            # example of distanceMatrix\n",
    "                            # \n",
    "                            # [[ 0  1  2 10 11]\n",
    "                            # [ 1  0  1  9 10]\n",
    "                            # [ 2  1  0  8  9]\n",
    "                            # [10  9  8  0  1]\n",
    "                            # [11 10  9  1  0]]\n",
    "\n",
    "\n",
    "                    return result\n",
    "                return []\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HW2 : codes of homework 2\n",
    "\n",
    "    def findDistance (self,inputWord, baseWord ,printOff=True ): # this function is used to find distance of each word if posting list with input word\n",
    "        inWord = \" \"+inputWord\n",
    "        bWord = \" \"+baseWord\n",
    "        distanceMatrix = [[0]* len(bWord) for _ in range(len(inWord))] # quantified the matrix\n",
    "        for i in range(len(bWord)):\n",
    "            distanceMatrix[0][i]=i\n",
    "        for i in range(len(inWord)):\n",
    "            distanceMatrix[i][0]=i\n",
    "\n",
    "        for row in range(1,len(inWord)): # fill the matrix using dynamic programming algorithm\n",
    "            for column in range(1,len(bWord)):\n",
    "                deleteScore = distanceMatrix[row-1][column] +1\n",
    "                insertScore = distanceMatrix[row][column-1] +1\n",
    "                copyOrReplaceScore = distanceMatrix[row-1][column-1] \n",
    "                if bWord[column] != inWord[row]:\n",
    "                    copyOrReplaceScore+=1\n",
    "                distanceMatrix[row][column] = min(deleteScore , insertScore , copyOrReplaceScore)\n",
    "                self.debugPrint (f'min is {distanceMatrix[row][column]} for {(deleteScore , insertScore , copyOrReplaceScore)} {row}, {column}' ,printOff=printOff)\n",
    " \n",
    "        self.debugPrint(np.array(distanceMatrix),printOff=printOff)\n",
    "        return distanceMatrix[-1][-1] # return the result\n",
    "\n",
    "\n",
    "\n",
    "    def spellCheckingSingleWord (self,inputWord,printOff=True): # call \"def findDistance ()\" for all words in posting list with input word\n",
    "        index = self.searchPostingList(inputWord) # check if spell is correct (word exists in posting list)\n",
    "        if (index< len(self.postingList)):\n",
    "            if (self.postingList[index][\"word\"] == inputWord):\n",
    "                return [inputWord]\n",
    "            \n",
    "        allWords = [item['word'] for item in self.postingList]\n",
    "        distances = [[] for _ in range(100) ]\n",
    "        for baseWord in allWords:\n",
    "            currentDistance = self.findDistance(inputWord, baseWord ,printOff=True) # call \"def findDistance ()\" for all words in posting list with input \n",
    "            distances[currentDistance].append(baseWord) # add new distance to distances list\n",
    "        self.debugPrint (distances,isMatrix=True, printOff=printOff)\n",
    "        for item in distances:\n",
    "            if(len(item)):\n",
    "                return item\n",
    "        return []\n",
    "            \n",
    "\n",
    "    def spellCheckingExpression(self, expression, printOff = True): # split the expression and check spell for all parts the concat them together\n",
    "        # tokenize input and remove stop words and ponctuations\n",
    "        tokenizedExpression= [word for word in word_tokenize(expression.lower(),preserve_line=False) if word not in self.stop] \n",
    "        answersLists = [] # list of all near words for each token\n",
    "        for word in tokenizedExpression:\n",
    "            answersLists.append(self.spellCheckingSingleWord(word))\n",
    "        self.debugPrint(answersLists, printOff=printOff)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for ls in answersLists: # multiply lists founded before\n",
    "            temp =[]\n",
    "            if (len(results)):\n",
    "                for wordResult in results:\n",
    "                    for word in ls:\n",
    "                        temp.append(f'{wordResult} {word}')\n",
    "                results = temp\n",
    "            else :results = copy.copy(ls)\n",
    "\n",
    "        self.debugPrint(results, printOff=printOff)\n",
    "        return results\n",
    "    \n",
    "\n",
    "    def findQueryWord (self, word ) : # I do my wild search in my tree class\n",
    "        wildcardFind = self.tryTree.find(word)\n",
    "        return wildcardFind\n",
    "\n",
    "\n",
    "    def findQuery (self, query ,printQueries=True) :\n",
    "        splitQuery = query.lower().split()\n",
    "        if ('*' not in query): # check if query is a wild card\n",
    "            # query does not contains any wildcard\n",
    "            if (len(splitQuery) == 3): # complicated query\n",
    "                # spell checking for both of sides\n",
    "                spellCheckFind1 = self.spellCheckingExpression(splitQuery[0]) \n",
    "                spellCheckFind2 = self.spellCheckingExpression(splitQuery[2])\n",
    "                results = set()\n",
    "                for spellItem1 in spellCheckFind1:\n",
    "                    for spellItem2 in spellCheckFind2: # make results\n",
    "                        if printQueries: print (f'query : {spellItem2} {splitQuery[1]} {spellItem1}')\n",
    "                        results = results.union(set(self.find(f'{spellItem2} {splitQuery[1]} {spellItem1}')))\n",
    "                    return list(results)\n",
    "                return self.find(query)\n",
    "            else: # simple query - just spellchecking \n",
    "                spellCheckFind = self.spellCheckingExpression(splitQuery[0])\n",
    "                results = set()\n",
    "                for item in spellCheckFind:\n",
    "                    results=results.union(set(self.find(item)))\n",
    "                return list(results)\n",
    "\n",
    "        # query contains some wildcards \n",
    "\n",
    "        if (len(splitQuery) == 3): # complicated query\n",
    "            if '*' not in splitQuery[0]: \n",
    "                splitQuery[0],splitQuery[2] = splitQuery[2],splitQuery[0] # move wildcard to left side\n",
    "\n",
    "            wildcardFind = self.tryTree.find(splitQuery[0]) # search wildcard\n",
    "            spellCheckFind = self.spellCheckingExpression(splitQuery[2]) # checking spell\n",
    "            self.debugPrint(wildcardFind)\n",
    "            results = set()\n",
    "            for spellItem in spellCheckFind: # make results\n",
    "                # search \n",
    "                for wildItem in wildcardFind:\n",
    "                    if printQueries: print (f'query : {wildItem} {splitQuery[1]} {spellItem}')\n",
    "                    results = results.union(set(self.find(f'{wildItem} {splitQuery[1]} {spellItem}')))\n",
    "                # print(f'{item} {splitQuery[1]} {splitQuery[2]}' ,set(self.find(f'{item} {splitQuery[1]} {splitQuery[2]}')) ,results)\n",
    "            self.debugPrint(wildcardFind,isMatrix=False)\n",
    "            self.debugPrint(splitQuery,isMatrix=True)\n",
    "            return list(results)\n",
    "        \n",
    "        else: # simple query with one wildcard\n",
    "            wildcardFind = self.tryTree.find(splitQuery[0]) \n",
    "            results = set()\n",
    "            for item in wildcardFind:\n",
    "                results = results.union(set(self.find(item))) # find all documents contain result of wild card searching\n",
    "            return list(results)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "            \n",
    "\n",
    "            \n",
    "    def debugPrint(self, value ,isMatrix = False, printOff=False):\n",
    "        if (self.debug and not printOff):\n",
    "            if (isMatrix):\n",
    "                for row in value:\n",
    "                    print(row)\n",
    "            else:\n",
    "                print(value)\n",
    "        \n",
    "\n",
    "\n",
    "    def prnt(self):\n",
    "        for i in self.postingList:\n",
    "            print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1 : document1.txt\n",
      "document 2 : document2.txt\n",
      "document 3 : document3.txt\n"
     ]
    }
   ],
   "source": [
    "test = searchEngine()\n",
    "\n",
    "test.input(['document1.txt','document2.txt','document3.txt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example test test test',\n",
       " 'several test test test',\n",
       " 'simple test test test',\n",
       " 'words test test test']"
      ]
     },
     "execution_count": 1040,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.spellCheckingExpression('amrali ani astam man',printOff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : example or test\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(test.findQuery( \"amir or ex*m*le\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : contains and several\n",
      "query : capabilities and several\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(test.findQuery('c*s and several'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : contains or several\n",
      "query : capabilities or several\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "print(test.findQuery('c*s or several'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : contains near/3 several\n",
      "query : capabilities near/3 several\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(test.findQuery('c*s* near/3 several'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : simple near/3 several\n",
      "query : several near/3 several\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(test.findQuery('s*l* near/3 several'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docs WildCards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['docs/Jerry Decided To Buy a Gun.txt', 'docs/Rentals at the Oceanside Community.txt', 'docs/Gasoline Prices Hit Record High.txt', 'docs/Cloning Pets.txt', 'docs/Crazy Housing Prices.txt', 'docs/Man Injured at Fast Food Place.txt', 'docs/A Festival of Books.txt', 'docs/Food Fight Erupted in Prison.txt', 'docs/Better To Be Unlucky.txt', 'docs/Sara Went Shopping.txt', 'docs/Freeway Chase Ends at Newsstand.txt', 'docs/Trees Are a Threat.txt', 'docs/A Murder-Suicide.txt', 'docs/Happy and Unhappy Renters.txt', 'docs/Pulling Out Nine Tons of Trash.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory_input = ['docs/'+path for path in os.listdir('docs') if path[-3:] == 'txt']\n",
    "print(directory_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1 : docs/Jerry Decided To Buy a Gun.txt\n",
      "document 2 : docs/Rentals at the Oceanside Community.txt\n",
      "document 3 : docs/Gasoline Prices Hit Record High.txt\n",
      "document 4 : docs/Cloning Pets.txt\n",
      "document 5 : docs/Crazy Housing Prices.txt\n",
      "document 6 : docs/Man Injured at Fast Food Place.txt\n",
      "document 7 : docs/A Festival of Books.txt\n",
      "document 8 : docs/Food Fight Erupted in Prison.txt\n",
      "document 9 : docs/Better To Be Unlucky.txt\n",
      "document 10 : docs/Sara Went Shopping.txt\n",
      "document 11 : docs/Freeway Chase Ends at Newsstand.txt\n",
      "document 12 : docs/Trees Are a Threat.txt\n",
      "document 13 : docs/A Murder-Suicide.txt\n",
      "document 14 : docs/Happy and Unhappy Renters.txt\n",
      "document 15 : docs/Pulling Out Nine Tons of Trash.txt\n"
     ]
    }
   ],
   "source": [
    "test_directory = searchEngine()\n",
    "test_directory.input(directory_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 7, 12, 13, 14]\n",
      "docs/Rentals at the Oceanside Community.txt\n",
      "docs/Gasoline Prices Hit Record High.txt\n",
      "docs/A Festival of Books.txt\n",
      "docs/Trees Are a Threat.txt\n",
      "docs/A Murder-Suicide.txt\n",
      "docs/Happy and Unhappy Renters.txt\n"
     ]
    }
   ],
   "source": [
    "result=test_directory.findQuery(\"People\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7]\n",
      "docs/Freeway Chase Ends at Newsstand.txt\n",
      "docs/A Festival of Books.txt\n"
     ]
    }
   ],
   "source": [
    "result=test_directory.findQuery(\"los\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : los and people\n",
      "[7]\n",
      "docs/A Festival of Books.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findQuery(\"People and Los\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : los or people\n",
      "[2, 3, 7, 11, 12, 13, 14]\n",
      "docs/Rentals at the Oceanside Community.txt\n",
      "docs/Gasoline Prices Hit Record High.txt\n",
      "docs/A Festival of Books.txt\n",
      "docs/Freeway Chase Ends at Newsstand.txt\n",
      "docs/Trees Are a Threat.txt\n",
      "docs/A Murder-Suicide.txt\n",
      "docs/Happy and Unhappy Renters.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findQuery(\"People OR Los\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : said near/4 piano\n",
      "[9]\n",
      "docs/Better To Be Unlucky.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findQuery(\"piano near/4 said\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : piano near/4 said\n",
      "[9]\n",
      "docs/Better To Be Unlucky.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findQuery(\"pia*o near/4 said\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : piano near/4 said\n",
      "[9]\n",
      "docs/Better To Be Unlucky.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findQuery(\"p*a*o near/4 said\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : pizza near/4 said\n",
      "query : pasadena near/4 said\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findQuery(\"p*a near/4 said\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : emphysema near/4 said\n",
      "query : pizza near/4 said\n",
      "query : pasadena near/4 said\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findQuery(\"*p*a near/4 said\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs Spell Checking\n",
    "### Input – Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "festival founders\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.spellCheckingExpression(\"festivsl funders\")\n",
    "for item in result:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canton\n",
      "court\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.spellCheckingExpression(\"contrnt\")\n",
    "for item in result:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs findWildCardWord\n",
    "### Input – Information Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nearby', 'nobody']\n"
     ]
    }
   ],
   "source": [
    "print( test_directory.findQueryWord(\"n*b*y\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs WildCardQuery\n",
    "### Input – Wildcard Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findQuery(\"exa*le AND content\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : less and popular\n",
      "query : line and popular\n",
      "query : left and popular\n",
      "query : late and popular\n",
      "query : lives and popular\n",
      "query : listening and popular\n",
      "query : lined and popular\n",
      "query : lake and popular\n",
      "query : liked and popular\n",
      "query : lived and popular\n",
      "query : letting and popular\n",
      "query : lines and popular\n",
      "query : lowest and popular\n",
      "query : let and popular\n",
      "query : loved and popular\n",
      "query : lottery and popular\n",
      "query : leading and popular\n",
      "query : lane and popular\n",
      "query : lose and popular\n",
      "query : least and popular\n",
      "query : life and popular\n",
      "query : listen and popular\n",
      "query : legal and popular\n",
      "query : libraries and popular\n",
      "query : love and popular\n",
      "query : little and popular\n",
      "query : like and popular\n",
      "query : later and popular\n",
      "query : leftovers and popular\n",
      "query : latest and popular\n",
      "query : lee and popular\n",
      "[8, 7]\n",
      "docs/Food Fight Erupted in Prison.txt\n",
      "docs/A Festival of Books.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findQuery(\"l*e* AND popila\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input – Information Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1 : document1.txt\n",
      "document 2 : document2.txt\n",
      "document 3 : document3.txt\n"
     ]
    }
   ],
   "source": [
    "test = searchEngine()\n",
    "\n",
    "test.input(['document1.txt','document2.txt','document3.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : example and content\n",
      "[2, 3]\n",
      "docs/Rentals at the Oceanside Community.txt\n",
      "docs/Gasoline Prices Hit Record High.txt\n"
     ]
    }
   ],
   "source": [
    "result = test.findQuery(\"exa*le AND contrnt\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
