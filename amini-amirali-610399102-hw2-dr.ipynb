{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an Information Retrieval System with Advanced Boolean Search\n",
    "\n",
    "## AmirAli Amini - 610399102\n",
    "\n",
    "#### HW2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# توضیحات مسئله و چالش ها و بهبود ها\n",
    "\n",
    "بزرگ ترین چالش مسئله ساختار پستینگ لیست که برای سیو کردن مقادیر استفاده میشود بود چرا که باید به صورتی ذخیره شوند که برای هر داکیومنت علاوه بر شامل بود کلمه ، ایندکس های آن کلمه هم ذخیره شود.\n",
    "\n",
    "در این مسئله از ساختاری سریع تر از لینک لیست برای دسترسی به دیتا استفاده کردم\n",
    "\n",
    "همچنین از باینری سرچ برای بهبود سرعت پیدا کردن دیتا استفاده میکنم\n",
    "\n",
    "میتوان تمام دیکشنری های استفاده شده را با ارایه دو بعدی جایگذین کرد که تاثیری در سرعت کد زده شده ندارد و تنها خوانایی کد پایین اورده میشود\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## کتابخانه ها \n",
    "\n",
    "###  from nltk import word_tokenize :\n",
    "از این کتابخانه برای تکنایز کردن داده ها به این دلیل که توکنایز کردن دیتا سریع تر میشه استفاده کردم\n",
    "\n",
    "###  from nltk.corpus import stopwords :\n",
    "از این کتابخانه برای دریافت استاپینگ ورد های زبان انگلیسی استفاده کردم\n",
    "\n",
    "###  import string:\n",
    "از این کتابخانه برای دریافت پانچویشن های زبان انگلیسی استفاده کردم\n",
    "\n",
    "###  import numpy as np:\n",
    "از این کتابخانه برای جمع یک عدد با تمام اعضای یک آرایه استفاده کردم\n",
    "\n",
    "###  import copy:\n",
    "از این کتابخانه برای دیپ کپی کردن ارایه استفاده کردم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/amirali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amirali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords # a library to tokenize input texts\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') # stopping word in English language\n",
    "\n",
    "import string # using to remove punctuation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code \n",
    "برای رسیدن به هدف سوال، یک \n",
    "\"posring list\"\n",
    "با ساختار\n",
    "list of {word : nameOfWord , docs :[list of {doc:nameOfDocument , indexes: indexes of the word if this document}]}\n",
    "میسازم که هر خانه متناظر با یک کلمه است و ساختاری به صورت گفته شده دارد\n",
    "\n",
    "در این ساختار داکیومنت هایی که دارای این کلمه هستند در این قسمت ذخیره میشووند به این صورت که ایندکس هایی که برابر آن کلمه در آن داکیومنت هستند نگهداری میشود\n",
    "\n",
    "دلیل نگه داری اندکس کلمات در داکیومنت ها این است که در آینده بتوان با متد \n",
    "\n",
    "near\n",
    "\n",
    "اختلاف مکانی دو کلمه را پیدا کرد\n",
    "\n",
    "\n",
    "کلیت ساخت پستینگ لیست به این صورت است که کلمات هر داکیومنت را نگاه میکند در پستینگ لیست پس از پیدا کردن کلمه مورد نظر، مقدار ایندکس آن در داکیومنت حاضر و شماره داکیومنت ثبت میشود\n",
    "\n",
    "توضیحات جزئی تر در کد به صورت کامنت نوشته شده است\n",
    "\n",
    "\n",
    "### input:\n",
    "تابع اینپوت ادرس داکیومنت ها را ورودی میگیرد و برای هر داکیومنت عملیات اضافه کردن به پستینگ لیست را انجام میدهد\n",
    "\n",
    "### findWord:\n",
    "این تابع ایندکس متناظر با کلمه سرج شده را خروجی میدهد\n",
    "\n",
    "### find:\n",
    "این در مرحله اول کویری داده شده را تجزیه میکند به ۳ کتگوری اصلی تقسیم بندی میشود\n",
    "\n",
    "- تک کلمه\n",
    "- بولین\n",
    "- همسایگی\n",
    "\n",
    "#### single word:\n",
    "برای تک کلمه به صورت مستثیم از تابع فایند ورد استفاده میکنم و ایندکس داکیومنت های آن را برمیگردانم\n",
    "\n",
    "#### boolean:\n",
    "برای بولین ها داکیومنت های هر کلمه را به همانند تک کلمه پیدا میکنم  \n",
    "\n",
    "##### AND:\n",
    "اشتراک لیست های بدست آمده را برمیگردانم\n",
    "\n",
    "##### OR:\n",
    "اجتماع لیست های بدست آمده را برمیگردانم\n",
    "\n",
    "\n",
    "### NEAR:\n",
    "برای کلمه اول و دوم برای هرکدام یک دیکشنری از داکیومنت های دارای آن کلمه میسازم که کلید آن اندکس داکیومنت و مقدار آن ایندکس کلمه در آن داکیومنت است\n",
    "\n",
    "برای هر دوداکیومنت مشترک در در دو کلمه ماتریس قدرمطلق فاصله متناظر با عضو های هردو را میسازم و مینیمم فاصله را بدست می آورم\n",
    "\n",
    "اگر کوچک تر از عدد خواسته شده بود آن داکیومنت را به جواب اضافه میکنم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input 1 , text2.txt : \n",
    "aa aa  aa bb bb bb zz zz ll mm mm  aa aa ll \n",
    "\n",
    "\n",
    "amir ali amini amir hossein hasani\n",
    "\n",
    "\n",
    "# input 2 , test.tst:\n",
    "amirali , amini, hastam salam. amirali , amirali aa bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node :\n",
    "    def __init__(self,isWord=False, myChar=None , myWord=None):\n",
    "        self.myChar = myChar\n",
    "        self.myWord = myWord\n",
    "        self.nodes = {}\n",
    "        self.seenChars = []\n",
    "        self.isAWrod = isWord\n",
    "    \n",
    "    def addNode(self ,char , isWord = False):\n",
    "        if (char not in self.seenChars):\n",
    "            newNode = node(myChar=char , myWord=self.myWord+char if self.myWord else char , isWord=isWord)\n",
    "            self.nodes[char] = newNode\n",
    "            self.seenChars.append(char)\n",
    "            return newNode\n",
    "        if isWord:\n",
    "            self.nodes[char].makeWord()\n",
    "        return self.nodes[char]\n",
    "        \n",
    "    def makeWord(self):\n",
    "        self.isAWrod=True\n",
    "\n",
    "    def getWord (self):\n",
    "        return self.myWord\n",
    "    \n",
    "    def getIsWord (self):\n",
    "        return self.isAWrod\n",
    "\n",
    "    def getChar (self):\n",
    "        return self.myChar\n",
    "    \n",
    "    def includesChar(self, char):\n",
    "        return char in self.seenChars\n",
    "    \n",
    "    def getSeenNodes (self):\n",
    "        return [self.nodes[char] for char in self.seenChars]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class tryTree :\n",
    "    def __init__ (self):\n",
    "        self.root = node()\n",
    "\n",
    "    def insertWord(self, word):\n",
    "        currentNode  = self.root\n",
    "        for char in range(len(word)): \n",
    "            currentNode = currentNode.addNode(word[char] , isWord = char == len(word)-1)\n",
    "\n",
    "\n",
    "    def insertWordPermutation (self,inputWord):\n",
    "        word = inputWord+\"$\"\n",
    "        for index in range(len(word)):\n",
    "            self.insertWord(word[index:]+word[:index])\n",
    "\n",
    "    def BFS(self , node):\n",
    "        result = [node]\n",
    "        for child in node.getSeenNodes():\n",
    "            result+= self.BFS(child)\n",
    "        return result\n",
    "    \n",
    "    def getBFSWords (self , node):\n",
    "        ls = self.BFS(node)\n",
    "        return [node.getWord() for node in ls if node.getIsWord()]\n",
    "    \n",
    "    def prefixBFS (self, prefix= ''):\n",
    "        currentNode = self.root\n",
    "        for char in range(len(prefix)): \n",
    "            currentNode = currentNode.addNode(prefix[char])\n",
    "        ls = self.getBFSWords(currentNode)\n",
    "        return ls\n",
    "\n",
    "    \n",
    "    def find(self, inputWord):\n",
    "        word = inputWord\n",
    "        if ('*' not in word):\n",
    "            return []\n",
    "        # if inputWord[-1] != \"*\":\n",
    "        word +='$'\n",
    "        startCount = word.count('*')\n",
    "        if (startCount ==1):\n",
    "            index = word.find('*')\n",
    "            prefix = word[index+1:]+word[:index]\n",
    "            ls = self.prefixBFS(prefix=prefix)\n",
    "            st = set()\n",
    "            for w in ls :\n",
    "                index = w.find('$')\n",
    "                st.add(w[index+1:]+w[:index])\n",
    "            return list(st)\n",
    "        elif(startCount==2):\n",
    "            while (word[-1]!='*'):\n",
    "                word = word[-1:]+word[:-1]\n",
    "            splitWord = word[:-1].split('*')\n",
    "            if len(splitWord[0])< len(splitWord[1]):\n",
    "                splitWord[0], splitWord[1] = splitWord[1],splitWord[0]\n",
    "            \n",
    "            ls = [i for i in self.prefixBFS(prefix=splitWord[0]) if splitWord[1] in i]\n",
    "\n",
    "            st = set()\n",
    "            for word in ls :\n",
    "                index = word.find('$')\n",
    "                st.add(word[index+1:]+word[:index])\n",
    "            return list(st)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def prnt (self,node=None):\n",
    "        root = self.root\n",
    "        if (node):\n",
    "            root = node\n",
    "        print (self.getBFSWords(root) )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class searchEngine:\n",
    "    def __init__(self , debug = False) -> None: # constructor of class\n",
    "        self.debug = debug\n",
    "        self.postingList =[]\n",
    "        self.files=[] \n",
    "        self.stop = set(stopwords.words('english') + list(string.punctuation)) # all extra expression which should ignore\n",
    "        self.tryTree = tryTree()\n",
    "\n",
    "        # structure of postingList : list of {word : nameOfWord , docs :[list of {doc:nameOfDocument , indexes: indexes of the word if this document}]}\n",
    "\n",
    "\n",
    "    # binary search to find a word in posting list\n",
    "    def searchPostingList(self, word):\n",
    "        s= 0 \n",
    "        e = len(self.postingList)\n",
    "        if e <=0 :\n",
    "            return 0\n",
    "        e-=1\n",
    "        while (1):\n",
    "            if (e-s < 2):\n",
    "                if (self.postingList[e][\"word\"] < word):\n",
    "                    return e+1\n",
    "                if (self.postingList[e][\"word\"] == word):\n",
    "                    return e\n",
    "                if (self.postingList[s][\"word\"] >= word):\n",
    "                    return s\n",
    "\n",
    "                return e\n",
    "            mid = (s+e)/2\n",
    "            mid = int(mid)\n",
    "            if (word<self.postingList[mid][\"word\"]):\n",
    "                e=mid\n",
    "            elif (word> self.postingList[mid][\"word\"]):\n",
    "                s = mid\n",
    "            else :\n",
    "                return mid\n",
    "            \n",
    "        # {word:str, indexes:list(int)}\n",
    "    # binary search to find a word in each dictionary\n",
    "    def searchDictionary(self, word,ls):\n",
    "        s= 0 \n",
    "        e = len(ls)\n",
    "        if e <=0 :\n",
    "            return 0\n",
    "        e-=1\n",
    "        while (1):\n",
    "            if (e-s < 2):\n",
    "                if (ls[e][\"doc\"] < word):\n",
    "                    return e+1\n",
    "                if (ls[e][\"doc\"] == word):\n",
    "                    return e\n",
    "                if (ls[s][\"doc\"] >= word):\n",
    "                    return s\n",
    "\n",
    "                return e\n",
    "            mid = (s+e)/2\n",
    "            mid = int(mid)\n",
    "            if (word<ls[mid][\"doc\"]):\n",
    "                e=mid\n",
    "            elif (word> ls[mid][\"doc\"]):\n",
    "                s = mid\n",
    "            else :\n",
    "                return mid\n",
    "\n",
    "\n",
    "            \n",
    "        # {doc:number, indexes:list(int)}\n",
    "\n",
    "\n",
    "\n",
    "    def addToPostingList(self, tokenizedText: list[str],docIndex:int): # add tokenized word in posting list \n",
    "        for i in range(len(tokenizedText)):\n",
    "            word = tokenizedText[i]\n",
    "            index = self.searchPostingList(word) # find index of word in posting list\n",
    "            if (len(self.postingList)>index): # check if index is not larger than posting list (if word is bigger that all words, search function returns len(postingList)+1)\n",
    "                if (self.postingList[index][\"word\"] == word): # check if index is the index of the word\n",
    "                    if (self.postingList[index][\"docs\"][-1][\"doc\"] == docIndex): # if we have already added the document index \n",
    "                        self.postingList[index][\"docs\"][-1][\"indexes\"].append(i) # as we read tokens in order of their index, we need to add token in end of the list\n",
    "\n",
    "                    else:\n",
    "                        self.postingList[index][\"docs\"].append({\"doc\":docIndex,\"indexes\":[i] }) # if we have not already added the document and dou to the fact that they are read in order of their index, we can easily add append new one in end of the list \n",
    "\n",
    "                else :\n",
    "                    self.postingList[index:index]= [({\"word\":word , \"docs\":[{\"doc\":docIndex , \"indexes\":[i]}]})] # word is bigger that all other words => we can append it to end of the list\n",
    "\n",
    "            else :\n",
    "                self.postingList.append({\"word\":word , \"docs\":[{\"doc\":docIndex , \"indexes\":[i]}]}) # we have not already added the word and dou to the fact that they are read in order of their index, we can easily add append new one in end of the list\n",
    "\n",
    "            self.tryTree.insertWordPermutation(word)\n",
    "\n",
    "\n",
    "    def input (self, filePath: list[str]): # input paths of inputs\n",
    "        for i in range(len(filePath)): # for files in input\n",
    "            file = open(filePath[i],'r',encoding='cp1252') # open the file\n",
    "            text = file.read() # read the file\n",
    "            file.close()  # close the file\n",
    "            # tokenize text and ignore stopping words using nltk library \n",
    "            tokenizedText = [word for word in word_tokenize(text.lower(),preserve_line=False) if word not in self.stop] \n",
    "            print (f'document {i+1} : {filePath[i]}')\n",
    "            # print(tokenizedText)\n",
    "            self.addToPostingList(tokenizedText , i+1) # i indicates to index of document we are reading\n",
    "        \n",
    "\n",
    "\n",
    "    def findWord(self, word): # this function use our binary search function to find word in posting list and if the word is not included in the list, returns -1\n",
    "        index = self.searchPostingList(word)\n",
    "        if (index< len(self.postingList)):\n",
    "            if (self.postingList[index][\"word\"] == word):\n",
    "                return index # real index of the word\n",
    "        return -1 # word is not in the posting list\n",
    "\n",
    "\n",
    "    def find(self , query:str): # split the query and find the result \n",
    "        splitQuery = query.lower().split()\n",
    "        if (len(splitQuery)==1): # query is only one word\n",
    "            index = self.findWord(splitQuery[0])\n",
    "            if (index>-1):\n",
    "                return [ i[\"doc\"] for i in self.postingList[index][\"docs\"]]\n",
    "            return []\n",
    "\n",
    "        else: \n",
    "            index1 = self.findWord(splitQuery[0])\n",
    "            index2 = self.findWord(splitQuery[2])\n",
    "            if splitQuery[1] in [\"and\" ,\"or\", \"AND\", \"OR\"]: # boolean condition\n",
    "                \n",
    "                if (splitQuery[1] in [\"and\",\"AND\"]): # and condition\n",
    "                    if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                        docs1 =  set([ i[\"doc\"] for i in self.postingList[index1][\"docs\"]]) # find document of word one \n",
    "                        docs2 =  set([ i[\"doc\"] for i in self.postingList[index2][\"docs\"]]) # find document of word two \n",
    "                        # print(self.postingList[index1][\"docs\"] , self.postingList[index2][\"docs\"])\n",
    "                        return list(docs1.intersection(docs2)) # make intersection of two results\n",
    "                    return []\n",
    "                    \n",
    "                if (splitQuery[1] in [\"or\",\"OR\"]): # or condition\n",
    "                    if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                        docs1 =  set([ i[\"doc\"] for i in self.postingList[index1][\"docs\"]])\n",
    "                        docs2 =  set([ i[\"doc\"] for i in self.postingList[index2][\"docs\"]])\n",
    "                        # print(self.postingList[index1][\"docs\"] , self.postingList[index2][\"docs\"])\n",
    "                        return list(docs1.union(docs2))\n",
    "                    if(index1!=-1): return [ i[\"doc\"] for i in self.postingList[index1][\"docs\"]] # check if there is result for first word\n",
    "                    return [ i[\"doc\"] for i in self.postingList[index2][\"docs\"]] # check if there is result for second word\n",
    "\n",
    "            else : # near condition\n",
    "                nearNumber = int(splitQuery[1].split('/')[1]) # find near \n",
    "                if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                    docs1 =   {i[\"doc\"]:i[\"indexes\"] for i in self.postingList[index1][\"docs\"]} # find document of word one and make dictionary for result\n",
    "                    docs2 =   {i[\"doc\"]:i[\"indexes\"] for i in self.postingList[index2][\"docs\"]} # find document of word two and make dictionary for result\n",
    "\n",
    "                    result = [] \n",
    "                    keysOfDocs2 = docs2.keys()\n",
    "                    for key, value in docs1.items():\n",
    "                        if (key in keysOfDocs2):\n",
    "                            d1 = np.array(value) # indexes of word in first document\n",
    "                            d2 = np.array(copy.deepcopy(docs2[key])) # indexes of word in second document\n",
    "                            distanceMatrix = np.array([[abs(i - j )for i in d1] for j in d2]) # distance matrix\n",
    "                            if (distanceMatrix.min() <= nearNumber): # check validation\n",
    "                                result.append(key)\n",
    "\n",
    "\n",
    "                            # example of distanceMatrix\n",
    "                            # \n",
    "                            # [[ 0  1  2 10 11]\n",
    "                            # [ 1  0  1  9 10]\n",
    "                            # [ 2  1  0  8  9]\n",
    "                            # [10  9  8  0  1]\n",
    "                            # [11 10  9  1  0]]\n",
    "\n",
    "\n",
    "                    return result\n",
    "                return []\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HW2 : codes of homework 2\n",
    "\n",
    "    def findDistance (self,inputWord, baseWord ,printOff=True ): # this function is used to find distance of each word if posting list with input word\n",
    "        inWord = \" \"+inputWord\n",
    "        bWord = \" \"+baseWord\n",
    "        distanceMatrix = [[0]* len(bWord) for _ in range(len(inWord))] # quantified the matrix\n",
    "        for i in range(len(bWord)):\n",
    "            distanceMatrix[0][i]=i\n",
    "        for i in range(len(inWord)):\n",
    "            distanceMatrix[i][0]=i\n",
    "\n",
    "        for row in range(1,len(inWord)): # fill the matrix using dynamic programming algorithm\n",
    "            for column in range(1,len(bWord)):\n",
    "                deleteScore = distanceMatrix[row-1][column] +1\n",
    "                insertScore = distanceMatrix[row][column-1] +1\n",
    "                copyOrReplaceScore = distanceMatrix[row-1][column-1] \n",
    "                if bWord[column] != inWord[row]:\n",
    "                    copyOrReplaceScore+=1\n",
    "                distanceMatrix[row][column] = min(deleteScore , insertScore , copyOrReplaceScore)\n",
    "                self.debugPrint (f'min is {distanceMatrix[row][column]} for {(deleteScore , insertScore , copyOrReplaceScore)} {row}, {column}' ,printOff=printOff)\n",
    " \n",
    "        self.debugPrint(np.array(distanceMatrix),printOff=printOff)\n",
    "        return distanceMatrix[-1][-1]\n",
    "\n",
    "\n",
    "\n",
    "    def spellCheckingSingleWord (self,inputWord,printOff=True): # call \"def findDistance ()\" for all words in posting list with input word\n",
    "        index = self.searchPostingList(inputWord)\n",
    "        if (index< len(self.postingList)):\n",
    "            if (self.postingList[index][\"word\"] == inputWord):\n",
    "                return [inputWord]\n",
    "            \n",
    "        allWords = [item['word'] for item in self.postingList]\n",
    "        distances = [[] for _ in range(100) ]\n",
    "        for baseWord in allWords:\n",
    "            currentDistance = self.findDistance(inputWord, baseWord ,printOff=True)\n",
    "            distances[currentDistance].append(baseWord)\n",
    "        self.debugPrint (distances,isMatrix=True, printOff=printOff)\n",
    "        for item in distances:\n",
    "            if(len(item)):\n",
    "                return item\n",
    "        return []\n",
    "            \n",
    "\n",
    "    def spellCheckingExpression(self, expression, printOff = True):\n",
    "        tokenizedExpression= [word for word in word_tokenize(expression.lower(),preserve_line=False) if word not in self.stop] \n",
    "        answersLists = []\n",
    "        for word in tokenizedExpression:\n",
    "            answersLists.append(self.spellCheckingSingleWord(word))\n",
    "        self.debugPrint(answersLists, printOff=printOff)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for ls in answersLists:\n",
    "            temp =[]\n",
    "            if (len(results)):\n",
    "                for wordResult in results:\n",
    "                    for word in ls:\n",
    "                        temp.append(f'{wordResult} {word}')\n",
    "                results = temp\n",
    "            else :results = copy.copy(ls)\n",
    "\n",
    "        self.debugPrint(results, printOff=printOff)\n",
    "        return results\n",
    "    \n",
    "\n",
    "    def findWildCardWord (self, word ) :\n",
    "        wildcardFind = self.tryTree.find(word)\n",
    "        return wildcardFind\n",
    "\n",
    "\n",
    "    def findWildCard (self, query ,printQueries=True) :\n",
    "        if ('*' not in query):\n",
    "            return self.find(query)\n",
    "        splitQuery = query.lower().split()\n",
    "        if (len(splitQuery) == 3):\n",
    "            if '*' not in splitQuery[0]:\n",
    "                splitQuery[0],splitQuery[2] = splitQuery[2],splitQuery[0]\n",
    "\n",
    "            wildcardFind = self.tryTree.find(splitQuery[0])\n",
    "            spellCheckFind = self.spellCheckingExpression(splitQuery[2])\n",
    "            self.debugPrint(wildcardFind)\n",
    "            results = set()\n",
    "            for spellItem in spellCheckFind:\n",
    "                for wildItem in wildcardFind:\n",
    "                    if printQueries: print (f'query : {wildItem} {splitQuery[1]} {spellItem}')\n",
    "                    results = results.union(set(self.find(f'{wildItem} {splitQuery[1]} {spellItem}')))\n",
    "                # print(f'{item} {splitQuery[1]} {splitQuery[2]}' ,set(self.find(f'{item} {splitQuery[1]} {splitQuery[2]}')) ,results)\n",
    "            self.debugPrint(wildcardFind,isMatrix=False)\n",
    "            self.debugPrint(splitQuery,isMatrix=True)\n",
    "            return list(results)\n",
    "        \n",
    "        else:\n",
    "            wildcardFind = self.tryTree.find(splitQuery[0])\n",
    "            results = set()\n",
    "            for item in wildcardFind:\n",
    "                results.union(set(self.find(item)))\n",
    "            return list(results)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "            \n",
    "\n",
    "            \n",
    "    def debugPrint(self, value ,isMatrix = False, printOff=False):\n",
    "        if (self.debug and not printOff):\n",
    "            if (isMatrix):\n",
    "                for row in value:\n",
    "                    print(row)\n",
    "            else:\n",
    "                print(value)\n",
    "        \n",
    "\n",
    "\n",
    "    def prnt(self):\n",
    "        for i in self.postingList:\n",
    "            print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1 : document1.txt\n",
      "document 2 : document2.txt\n",
      "document 3 : document3.txt\n"
     ]
    }
   ],
   "source": [
    "test = searchEngine()\n",
    "\n",
    "test.input(['document1.txt','document2.txt','document3.txt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example test test test',\n",
       " 'several test test test',\n",
       " 'simple test test test',\n",
       " 'words test test test']"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.spellCheckingExpression('amrali ani astam man',printOff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : example or test\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(test.findWildCard( \"amir or ex*m*le\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : contains and several\n",
      "query : capabilities and several\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(test.findWildCard('c*s and several'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : contains or several\n",
      "query : capabilities or several\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "print(test.findWildCard('c*s or several'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : contains near/3 several\n",
      "query : capabilities near/3 several\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(test.findWildCard('c*s* near/3 several'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : simple near/3 several\n",
      "query : several near/3 several\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(test.findWildCard('s*l* near/3 several'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docs WildCards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['docs/Jerry Decided To Buy a Gun.txt', 'docs/Rentals at the Oceanside Community.txt', 'docs/Gasoline Prices Hit Record High.txt', 'docs/Cloning Pets.txt', 'docs/Crazy Housing Prices.txt', 'docs/Man Injured at Fast Food Place.txt', 'docs/A Festival of Books.txt', 'docs/Food Fight Erupted in Prison.txt', 'docs/Better To Be Unlucky.txt', 'docs/Sara Went Shopping.txt', 'docs/Freeway Chase Ends at Newsstand.txt', 'docs/Trees Are a Threat.txt', 'docs/A Murder-Suicide.txt', 'docs/Happy and Unhappy Renters.txt', 'docs/Pulling Out Nine Tons of Trash.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory_input = ['docs/'+path for path in os.listdir('docs') if path[-3:] == 'txt']\n",
    "print(directory_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1 : docs/Jerry Decided To Buy a Gun.txt\n",
      "document 2 : docs/Rentals at the Oceanside Community.txt\n",
      "document 3 : docs/Gasoline Prices Hit Record High.txt\n",
      "document 4 : docs/Cloning Pets.txt\n",
      "document 5 : docs/Crazy Housing Prices.txt\n",
      "document 6 : docs/Man Injured at Fast Food Place.txt\n",
      "document 7 : docs/A Festival of Books.txt\n",
      "document 8 : docs/Food Fight Erupted in Prison.txt\n",
      "document 9 : docs/Better To Be Unlucky.txt\n",
      "document 10 : docs/Sara Went Shopping.txt\n",
      "document 11 : docs/Freeway Chase Ends at Newsstand.txt\n",
      "document 12 : docs/Trees Are a Threat.txt\n",
      "document 13 : docs/A Murder-Suicide.txt\n",
      "document 14 : docs/Happy and Unhappy Renters.txt\n",
      "document 15 : docs/Pulling Out Nine Tons of Trash.txt\n"
     ]
    }
   ],
   "source": [
    "test_directory = searchEngine()\n",
    "test_directory.input(directory_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 7, 12, 13, 14]\n",
      "docs/Rentals at the Oceanside Community.txt\n",
      "docs/Gasoline Prices Hit Record High.txt\n",
      "docs/A Festival of Books.txt\n",
      "docs/Trees Are a Threat.txt\n",
      "docs/A Murder-Suicide.txt\n",
      "docs/Happy and Unhappy Renters.txt\n"
     ]
    }
   ],
   "source": [
    "result=test_directory.findWildCard(\"People\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 11]\n",
      "docs/A Festival of Books.txt\n",
      "docs/Freeway Chase Ends at Newsstand.txt\n"
     ]
    }
   ],
   "source": [
    "result=test_directory.findWildCard(\"Los\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n",
      "docs/A Festival of Books.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findWildCard(\"People and Los\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 7, 11, 12, 13, 14]\n",
      "docs/Rentals at the Oceanside Community.txt\n",
      "docs/Gasoline Prices Hit Record High.txt\n",
      "docs/A Festival of Books.txt\n",
      "docs/Freeway Chase Ends at Newsstand.txt\n",
      "docs/Trees Are a Threat.txt\n",
      "docs/A Murder-Suicide.txt\n",
      "docs/Happy and Unhappy Renters.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findWildCard(\"People OR Los\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n",
      "docs/Better To Be Unlucky.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findWildCard(\"piano near/4 said\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : piano near/4 said\n",
      "[9]\n",
      "docs/Better To Be Unlucky.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findWildCard(\"pia*o near/4 said\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : piano near/4 said\n",
      "[9]\n",
      "docs/Better To Be Unlucky.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findWildCard(\"p*a*o near/4 said\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : pizza near/4 said\n",
      "query : pasadena near/4 said\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findWildCard(\"p*a near/4 said\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : emphysema near/4 said\n",
      "query : pizza near/4 said\n",
      "query : pasadena near/4 said\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findWildCard(\"*p*a near/4 said\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs Spell Checking\n",
    "### Input – Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "festival founders\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.spellCheckingExpression(\"festivsl funders\")\n",
    "for item in result:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canton\n",
      "court\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.spellCheckingExpression(\"contrnt\")\n",
    "for item in result:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs findWildCardWord\n",
    "### Input – Information Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nearby', 'nobody']\n"
     ]
    }
   ],
   "source": [
    "print( test_directory.findWildCardWord(\"n*b*y\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs WildCardQuery\n",
    "### Input – Wildcard Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findWildCard(\"exa*le AND content\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : life and popular\n",
      "query : love and popular\n",
      "query : little and popular\n",
      "query : line and popular\n",
      "query : like and popular\n",
      "query : lake and popular\n",
      "query : late and popular\n",
      "query : lane and popular\n",
      "query : lose and popular\n",
      "query : lee and popular\n",
      "[8, 7]\n",
      "docs/Food Fight Erupted in Prison.txt\n",
      "docs/A Festival of Books.txt\n"
     ]
    }
   ],
   "source": [
    "result = test_directory.findWildCard(\"l*e AND popila\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input – Information Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1 : document1.txt\n",
      "document 2 : document2.txt\n",
      "document 3 : document3.txt\n"
     ]
    }
   ],
   "source": [
    "test = searchEngine()\n",
    "\n",
    "test.input(['document1.txt','document2.txt','document3.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : example and content\n",
      "[2, 3]\n",
      "docs/Rentals at the Oceanside Community.txt\n",
      "docs/Gasoline Prices Hit Record High.txt\n"
     ]
    }
   ],
   "source": [
    "result = test.findWildCard(\"exa*le AND content\")\n",
    "print(result)\n",
    "for file in [directory_input[index-1] for index in result]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
