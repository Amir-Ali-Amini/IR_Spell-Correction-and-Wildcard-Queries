{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an Information Retrieval System with Advanced Boolean Search\n",
    "\n",
    "## AmirAli Amini - 610399102\n",
    "\n",
    "#### HW2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# توضیحات مسئله و چالش ها و بهبود ها\n",
    "\n",
    "بزرگ ترین چالش مسئله ساختار پستینگ لیست که برای سیو کردن مقادیر استفاده میشود بود چرا که باید به صورتی ذخیره شوند که برای هر داکیومنت علاوه بر شامل بود کلمه ، ایندکس های آن کلمه هم ذخیره شود.\n",
    "\n",
    "در این مسئله از ساختاری سریع تر از لینک لیست برای دسترسی به دیتا استفاده کردم\n",
    "\n",
    "همچنین از باینری سرچ برای بهبود سرعت پیدا کردن دیتا استفاده میکنم\n",
    "\n",
    "میتوان تمام دیکشنری های استفاده شده را با ارایه دو بعدی جایگذین کرد که تاثیری در سرعت کد زده شده ندارد و تنها خوانایی کد پایین اورده میشود\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## کتابخانه ها \n",
    "\n",
    "###  from nltk import word_tokenize :\n",
    "از این کتابخانه برای تکنایز کردن داده ها به این دلیل که توکنایز کردن دیتا سریع تر میشه استفاده کردم\n",
    "\n",
    "###  from nltk.corpus import stopwords :\n",
    "از این کتابخانه برای دریافت استاپینگ ورد های زبان انگلیسی استفاده کردم\n",
    "\n",
    "###  import string:\n",
    "از این کتابخانه برای دریافت پانچویشن های زبان انگلیسی استفاده کردم\n",
    "\n",
    "###  import numpy as np:\n",
    "از این کتابخانه برای جمع یک عدد با تمام اعضای یک آرایه استفاده کردم\n",
    "\n",
    "###  import copy:\n",
    "از این کتابخانه برای دیپ کپی کردن ارایه استفاده کردم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/amirali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amirali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords # a library to tokenize input texts\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') # stopping word in English language\n",
    "\n",
    "import string # using to remove punctuation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code \n",
    "برای رسیدن به هدف سوال، یک \n",
    "\"posring list\"\n",
    "با ساختار\n",
    "list of {word : nameOfWord , docs :[list of {doc:nameOfDocument , indexes: indexes of the word if this document}]}\n",
    "میسازم که هر خانه متناظر با یک کلمه است و ساختاری به صورت گفته شده دارد\n",
    "\n",
    "در این ساختار داکیومنت هایی که دارای این کلمه هستند در این قسمت ذخیره میشووند به این صورت که ایندکس هایی که برابر آن کلمه در آن داکیومنت هستند نگهداری میشود\n",
    "\n",
    "دلیل نگه داری اندکس کلمات در داکیومنت ها این است که در آینده بتوان با متد \n",
    "\n",
    "near\n",
    "\n",
    "اختلاف مکانی دو کلمه را پیدا کرد\n",
    "\n",
    "\n",
    "کلیت ساخت پستینگ لیست به این صورت است که کلمات هر داکیومنت را نگاه میکند در پستینگ لیست پس از پیدا کردن کلمه مورد نظر، مقدار ایندکس آن در داکیومنت حاضر و شماره داکیومنت ثبت میشود\n",
    "\n",
    "توضیحات جزئی تر در کد به صورت کامنت نوشته شده است\n",
    "\n",
    "\n",
    "### input:\n",
    "تابع اینپوت ادرس داکیومنت ها را ورودی میگیرد و برای هر داکیومنت عملیات اضافه کردن به پستینگ لیست را انجام میدهد\n",
    "\n",
    "### findWord:\n",
    "این تابع ایندکس متناظر با کلمه سرج شده را خروجی میدهد\n",
    "\n",
    "### find:\n",
    "این در مرحله اول کویری داده شده را تجزیه میکند به ۳ کتگوری اصلی تقسیم بندی میشود\n",
    "\n",
    "- تک کلمه\n",
    "- بولین\n",
    "- همسایگی\n",
    "\n",
    "#### single word:\n",
    "برای تک کلمه به صورت مستثیم از تابع فایند ورد استفاده میکنم و ایندکس داکیومنت های آن را برمیگردانم\n",
    "\n",
    "#### boolean:\n",
    "برای بولین ها داکیومنت های هر کلمه را به همانند تک کلمه پیدا میکنم  \n",
    "\n",
    "##### AND:\n",
    "اشتراک لیست های بدست آمده را برمیگردانم\n",
    "\n",
    "##### OR:\n",
    "اجتماع لیست های بدست آمده را برمیگردانم\n",
    "\n",
    "\n",
    "### NEAR:\n",
    "برای کلمه اول و دوم برای هرکدام یک دیکشنری از داکیومنت های دارای آن کلمه میسازم که کلید آن اندکس داکیومنت و مقدار آن ایندکس کلمه در آن داکیومنت است\n",
    "\n",
    "برای هر دوداکیومنت مشترک در در دو کلمه ماتریس قدرمطلق فاصله متناظر با عضو های هردو را میسازم و مینیمم فاصله را بدست می آورم\n",
    "\n",
    "اگر کوچک تر از عدد خواسته شده بود آن داکیومنت را به جواب اضافه میکنم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input 1 , text2.txt : \n",
    "aa aa  aa bb bb bb zz zz ll mm mm  aa aa ll \n",
    "\n",
    "\n",
    "amir ali amini amir hossein hasani\n",
    "\n",
    "\n",
    "# input 2 , test.tst:\n",
    "amirali , amini, hastam salam. amirali , amirali aa bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class searchEngine:\n",
    "    def __init__(self , debug = False) -> None: # constructor of class\n",
    "        self.debug = debug\n",
    "        self.postingList =[]\n",
    "        self.files=[] \n",
    "        self.stop = set(stopwords.words('english') + list(string.punctuation)) # all extra expression which should ignore\n",
    "\n",
    "        # structure of postingList : list of {word : nameOfWord , docs :[list of {doc:nameOfDocument , indexes: indexes of the word if this document}]}\n",
    "\n",
    "\n",
    "    # binary search to find a word in posting list\n",
    "    def searchPostingList(self, word):\n",
    "        s= 0 \n",
    "        e = len(self.postingList)\n",
    "        if e <=0 :\n",
    "            return 0\n",
    "        e-=1\n",
    "        while (1):\n",
    "            if (e-s < 2):\n",
    "                if (self.postingList[e][\"word\"] < word):\n",
    "                    return e+1\n",
    "                if (self.postingList[e][\"word\"] == word):\n",
    "                    return e\n",
    "                if (self.postingList[s][\"word\"] >= word):\n",
    "                    return s\n",
    "\n",
    "                return e\n",
    "            mid = (s+e)/2\n",
    "            mid = int(mid)\n",
    "            if (word<self.postingList[mid][\"word\"]):\n",
    "                e=mid\n",
    "            elif (word> self.postingList[mid][\"word\"]):\n",
    "                s = mid\n",
    "            else :\n",
    "                return mid\n",
    "            \n",
    "        # {word:str, indexes:list(int)}\n",
    "    # binary search to find a word in each dictionary\n",
    "    def searchDictionary(self, word,ls):\n",
    "        s= 0 \n",
    "        e = len(ls)\n",
    "        if e <=0 :\n",
    "            return 0\n",
    "        e-=1\n",
    "        while (1):\n",
    "            if (e-s < 2):\n",
    "                if (ls[e][\"doc\"] < word):\n",
    "                    return e+1\n",
    "                if (ls[e][\"doc\"] == word):\n",
    "                    return e\n",
    "                if (ls[s][\"doc\"] >= word):\n",
    "                    return s\n",
    "\n",
    "                return e\n",
    "            mid = (s+e)/2\n",
    "            mid = int(mid)\n",
    "            if (word<ls[mid][\"doc\"]):\n",
    "                e=mid\n",
    "            elif (word> ls[mid][\"doc\"]):\n",
    "                s = mid\n",
    "            else :\n",
    "                return mid\n",
    "\n",
    "\n",
    "            \n",
    "        # {doc:number, indexes:list(int)}\n",
    "\n",
    "\n",
    "\n",
    "    def addToPostingList(self, tokenizedText: list[str],docIndex:int): # add tokenized word in posting list \n",
    "        for i in range(len(tokenizedText)):\n",
    "            word = tokenizedText[i]\n",
    "            index = self.searchPostingList(word) # find index of word in posting list\n",
    "            if (len(self.postingList)>index): # check if index is not larger than posting list (if word is bigger that all words, search function returns len(postingList)+1)\n",
    "                if (self.postingList[index][\"word\"] == word): # check if index is the index of the word\n",
    "                    if (self.postingList[index][\"docs\"][-1][\"doc\"] == docIndex): # if we have already added the document index \n",
    "                        self.postingList[index][\"docs\"][-1][\"indexes\"].append(i) # as we read tokens in order of their index, we need to add token in end of the list\n",
    "\n",
    "                    else:\n",
    "                        self.postingList[index][\"docs\"].append({\"doc\":docIndex,\"indexes\":[i] }) # if we have not already added the document and dou to the fact that they are read in order of their index, we can easily add append new one in end of the list \n",
    "\n",
    "                else :\n",
    "                    self.postingList[index:index]= [({\"word\":word , \"docs\":[{\"doc\":docIndex , \"indexes\":[i]}]})] # word is bigger that all other words => we can append it to end of the list\n",
    "\n",
    "            else :\n",
    "                self.postingList.append({\"word\":word , \"docs\":[{\"doc\":docIndex , \"indexes\":[i]}]}) # we have not already added the word and dou to the fact that they are read in order of their index, we can easily add append new one in end of the list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def input (self, filePath: list[str]): # input paths of inputs\n",
    "        for i in range(len(filePath)): # for files in input\n",
    "            file = open(filePath[i],'r',encoding='cp1252') # open the file\n",
    "            text = file.read() # read the file\n",
    "            file.close()  # close the file\n",
    "            # tokenize text and ignore stopping words using nltk library \n",
    "            tokenizedText = [word for word in word_tokenize(text.lower(),preserve_line=False) if word not in self.stop] \n",
    "            print (f'document {i+1} : {filePath[i]}')\n",
    "            # print(tokenizedText)\n",
    "            self.addToPostingList(tokenizedText , i+1) # i indicates to index of document we are reading\n",
    "        \n",
    "\n",
    "\n",
    "    def findWord(self, word): # this function use our binary search function to find word in posting list and if the word is not included in the list, returns -1\n",
    "        index = self.searchPostingList(word)\n",
    "        if (index< len(self.postingList)):\n",
    "            if (self.postingList[index][\"word\"] == word):\n",
    "                return index # real index of the word\n",
    "        return -1 # word is not in the posting list\n",
    "\n",
    "\n",
    "    def find(self , query:str): # split the query and find the result \n",
    "        splitQuery = query.lower().split()\n",
    "        if (len(splitQuery)==1): # query is only one word\n",
    "            index = self.findWord(splitQuery[0])\n",
    "            if (index>-1):\n",
    "                return [ i[\"doc\"] for i in self.postingList[index][\"docs\"]]\n",
    "            return []\n",
    "\n",
    "        else: \n",
    "            index1 = self.findWord(splitQuery[0])\n",
    "            index2 = self.findWord(splitQuery[2])\n",
    "            if splitQuery[1] in [\"and\" ,\"or\", \"AND\", \"OR\"]: # boolean condition\n",
    "                \n",
    "                if (splitQuery[1] in [\"and\",\"AND\"]): # and condition\n",
    "                    if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                        docs1 =  set([ i[\"doc\"] for i in self.postingList[index1][\"docs\"]]) # find document of word one \n",
    "                        docs2 =  set([ i[\"doc\"] for i in self.postingList[index2][\"docs\"]]) # find document of word two \n",
    "                        # print(self.postingList[index1][\"docs\"] , self.postingList[index2][\"docs\"])\n",
    "                        return list(docs1.intersection(docs2)) # make intersection of two results\n",
    "                    return []\n",
    "                    \n",
    "                if (splitQuery[1] in [\"or\",\"OR\"]): # or condition\n",
    "                    if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                        docs1 =  set([ i[\"doc\"] for i in self.postingList[index1][\"docs\"]])\n",
    "                        docs2 =  set([ i[\"doc\"] for i in self.postingList[index2][\"docs\"]])\n",
    "                        # print(self.postingList[index1][\"docs\"] , self.postingList[index2][\"docs\"])\n",
    "                        return list(docs1.union(docs2))\n",
    "                    if(index1!=-1): return [ i[\"doc\"] for i in self.postingList[index1][\"docs\"]] # check if there is result for first word\n",
    "                    return [ i[\"doc\"] for i in self.postingList[index2][\"docs\"]] # check if there is result for second word\n",
    "\n",
    "            else : # near condition\n",
    "                nearNumber = int(splitQuery[1].split('/')[1]) # find near \n",
    "                if(index1!=-1 and index2!=-1): # check if there are result for both of words\n",
    "                    docs1 =   {i[\"doc\"]:i[\"indexes\"] for i in self.postingList[index1][\"docs\"]} # find document of word one and make dictionary for result\n",
    "                    docs2 =   {i[\"doc\"]:i[\"indexes\"] for i in self.postingList[index2][\"docs\"]} # find document of word two and make dictionary for result\n",
    "\n",
    "                    result = [] \n",
    "                    keysOfDocs2 = docs2.keys()\n",
    "                    for key, value in docs1.items():\n",
    "                        if (key in keysOfDocs2):\n",
    "                            d1 = np.array(value) # indexes of word in first document\n",
    "                            d2 = np.array(copy.deepcopy(docs2[key])) # indexes of word in second document\n",
    "                            distanceMatrix = np.array([[abs(i - j )for i in d1] for j in d2]) # distance matrix\n",
    "                            if (distanceMatrix.min() <= nearNumber): # check validation\n",
    "                                result.append(key)\n",
    "\n",
    "\n",
    "                            # example of distanceMatrix\n",
    "                            # \n",
    "                            # [[ 0  1  2 10 11]\n",
    "                            # [ 1  0  1  9 10]\n",
    "                            # [ 2  1  0  8  9]\n",
    "                            # [10  9  8  0  1]\n",
    "                            # [11 10  9  1  0]]\n",
    "\n",
    "\n",
    "                    return result\n",
    "                return []\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HW2\n",
    "\n",
    "    def findDistance (self,inputWord, baseWord ,printOff=True ):\n",
    "        inWord = \" \"+inputWord\n",
    "        bWord = \" \"+baseWord\n",
    "        distanceMatrix = [[0]* len(bWord) for _ in range(len(inWord))]\n",
    "        for i in range(len(bWord)):\n",
    "            distanceMatrix[0][i]=i\n",
    "        for i in range(len(inWord)):\n",
    "            distanceMatrix[i][0]=i\n",
    "\n",
    "        for row in range(1,len(inWord)):\n",
    "            for column in range(1,len(bWord)):\n",
    "                deleteScore = distanceMatrix[row-1][column] +1\n",
    "                insertScore = distanceMatrix[row][column-1] +1\n",
    "                copyOrReplaceScore = distanceMatrix[row-1][column-1] \n",
    "                if bWord[column] != inWord[row]:\n",
    "                    copyOrReplaceScore+=1\n",
    "                distanceMatrix[row][column] = min(deleteScore , insertScore , copyOrReplaceScore)\n",
    "                # self.debugPrint (f'min is {distanceMatrix[row][column]} for {(deleteScore , insertScore , copyOrReplaceScore)} {row}, {column}' ,printOff=printOff)\n",
    " \n",
    "        self.debugPrint(np.array(distanceMatrix),printOff=printOff)\n",
    "        return distanceMatrix[-1][-1]\n",
    "\n",
    "\n",
    "\n",
    "    def spellCheckingSingleWord (self,inputWord,printOff=True):\n",
    "        index = self.searchPostingList(inputWord)\n",
    "        if (index< len(self.postingList)):\n",
    "            if (self.postingList[index][\"word\"] == inputWord):\n",
    "                return [inputWord]\n",
    "            \n",
    "        allWords = [item['word'] for item in self.postingList]\n",
    "        distances = [[] for _ in range(100) ]\n",
    "        for baseWord in allWords:\n",
    "            currentDistance = self.findDistance(inputWord, baseWord ,printOff=True)\n",
    "            distances[currentDistance].append(baseWord)\n",
    "        self.debugPrint (distances,isMatrix=True, printOff=printOff)\n",
    "        for item in distances:\n",
    "            if(len(item)):\n",
    "                return item\n",
    "        return []\n",
    "            \n",
    "\n",
    "    def spellCheckingExpression(self, expression, printOff = True):\n",
    "        tokenizedExpression= [word for word in word_tokenize(expression.lower(),preserve_line=False) if word not in self.stop] \n",
    "        answersLists = []\n",
    "        for word in tokenizedExpression:\n",
    "            answersLists.append(self.spellCheckingSingleWord(word))\n",
    "        self.debugPrint(answersLists, printOff=printOff)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for ls in answersLists:\n",
    "            temp =[]\n",
    "            if (len(results)):\n",
    "                for wordResult in results:\n",
    "                    for word in ls:\n",
    "                        temp.append(f'{wordResult} {word}')\n",
    "                results = temp\n",
    "            else :results = copy.copy(ls)\n",
    "\n",
    "        self.debugPrint(results, printOff=printOff)\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "            \n",
    "\n",
    "            \n",
    "    def debugPrint(self, value ,isMatrix = False, printOff=False):\n",
    "        if (self.debug and not printOff):\n",
    "            if (isMatrix):\n",
    "                for row in value:\n",
    "                    print(row)\n",
    "            else:\n",
    "                print(value)\n",
    "        \n",
    "\n",
    "\n",
    "    def prnt(self):\n",
    "        for i in self.postingList:\n",
    "            print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1 : document1.txt\n",
      "document 2 : document2.txt\n",
      "document 3 : document3.txt\n"
     ]
    }
   ],
   "source": [
    "test = searchEngine(debug=True)\n",
    "\n",
    "test.input(['document1.txt','document2.txt','document3.txt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.spellCheckingExpression('amrali ani astam man',printOff=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
